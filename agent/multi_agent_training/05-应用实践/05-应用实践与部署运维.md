# ç¬¬äº”å¤©ï¼šåº”ç”¨å®è·µä¸éƒ¨ç½²è¿ç»´

## å­¦ä¹ ç›®æ ‡

- æŒæ¡å¤šæ™ºèƒ½ä½“ç³»ç»Ÿçš„å®Œæ•´å¼€å‘æµç¨‹
- å­¦ä¼šä¼ä¸šçº§éƒ¨ç½²ä¸è¿ç»´æœ€ä½³å®è·µ
- å…·å¤‡ç‹¬ç«‹æ„å»ºå¤šæ™ºèƒ½ä½“ç³»ç»Ÿçš„èƒ½åŠ›
- äº†è§£ç”Ÿäº§ç¯å¢ƒçš„ç›‘æ§ä¸ä¼˜åŒ–ç­–ç•¥
- æŒæ¡å®¹å™¨åŒ–å’Œäº‘åŸç”Ÿéƒ¨ç½²æŠ€æœ¯

## å‚è€ƒé¡¹ç›®

æœ¬è¯¾ç¨‹å°†å¸¦é¢†å­¦å‘˜å®Œæˆä¸€ä¸ªå®Œæ•´çš„ä¼ä¸šçº§å¤šæ™ºèƒ½ä½“ç³»ç»Ÿé¡¹ç›®ï¼Œä»éœ€æ±‚åˆ†æåˆ°éƒ¨ç½²è¿ç»´çš„å…¨æµç¨‹å®è·µã€‚

**ğŸ’¡ å®é™…é¡¹ç›®å‚è€ƒ**ï¼šæœ¬è¯¾ç¨‹åŸºäºé¡¹ç›®ä¸­çš„å®Œæ•´å®ç°è¿›è¡Œæ•™å­¦ï¼š

- `customer_service_system.py` - å®Œæ•´çš„ä¼ä¸šçº§å®¢æœç³»ç»Ÿå®ç°
- `main.py` - ç³»ç»Ÿå¯åŠ¨å’Œéƒ¨ç½²è„šæœ¬
- `docker/` - å®¹å™¨åŒ–éƒ¨ç½²é…ç½®
- `k8s/` - Kuberneteséƒ¨ç½²æ¸…å•
- `monitoring/` - ç›‘æ§å’Œå‘Šè­¦é…ç½®

**å®æˆ˜ç‰¹è‰²**ï¼šä»å®é™…ä¼ä¸šéœ€æ±‚å‡ºå‘ï¼Œæ¶µç›–å®Œæ•´çš„å¼€å‘ã€æµ‹è¯•ã€éƒ¨ç½²ã€ç›‘æ§ã€è¿ç»´æµç¨‹ï¼Œæä¾›ç”Ÿäº§çº§çš„è§£å†³æ–¹æ¡ˆã€‚

---

## 1. ç»¼åˆé¡¹ç›®å®æˆ˜

### 1.1 é¡¹ç›®éœ€æ±‚åˆ†æ

#### 1.1.1 åŸºäºå®é™…é¡¹ç›®çš„æ™ºèƒ½å®¢æœç³»ç»Ÿ

```python
**ä»£ç å¼•ç”¨**: å®Œæ•´çš„æ™ºèƒ½å®¢æœç³»ç»Ÿå®ç°è¯·å‚è€ƒ `multi_agent_system/src/applications/customer_service_system.py`

åŸºäºå®é™…é¡¹ç›®çš„æ™ºèƒ½å®¢æœç³»ç»Ÿéœ€æ±‚åˆ†æï¼š

**æ ¸å¿ƒæ•°æ®ç»“æ„**ï¼š
- `CustomerServicePriority`: å®¢æœä¼˜å…ˆçº§ç®¡ç†ï¼ˆä½ã€ä¸­ã€é«˜ã€ç´§æ€¥ï¼‰
- `TicketStatus`: å·¥å•çŠ¶æ€è·Ÿè¸ªï¼ˆå¼€æ”¾ã€è¿›è¡Œä¸­ã€å¾…å¤„ç†ã€å·²è§£å†³ã€å·²å…³é—­ï¼‰
- `CustomerSentiment`: å®¢æˆ·æƒ…ç»ªåˆ†æï¼ˆéå¸¸è´Ÿé¢åˆ°éå¸¸æ­£é¢ï¼‰
- `CustomerProfile`: å®¢æˆ·æ¡£æ¡ˆç®¡ç†ï¼ˆVIPç­‰çº§ã€å†å²è®°å½•ã€æ»¡æ„åº¦è¯„åˆ†ï¼‰
- `SupportTicket`: æ”¯æŒå·¥å•å®Œæ•´ç”Ÿå‘½å‘¨æœŸç®¡ç†

**åŠŸèƒ½éœ€æ±‚**ï¼š
- æ™ºèƒ½å¯¹è¯ï¼šå¤šè½®å¯¹è¯ï¼Œæ„å›¾è¯†åˆ«å‡†ç¡®ç‡ > 95%
- çŸ¥è¯†æ£€ç´¢ï¼šå‘é‡æ•°æ®åº“ + è¯­ä¹‰æœç´¢ï¼Œå“åº”æ—¶é—´ < 500ms
- å·¥å•å¤„ç†ï¼šå·¥ä½œæµå¼•æ“ + çŠ¶æ€æœºï¼Œå¤„ç†æ•ˆç‡æå‡ 50%
- æƒ…æ„Ÿåˆ†æï¼šæƒ…æ„Ÿè¯†åˆ«å‡†ç¡®ç‡ > 90%ï¼Œæ™ºèƒ½è½¬äººå·¥
- å¤šæ¸ é“æ¥å…¥ï¼šæ”¯æŒ 5+ æ¸ é“ï¼ˆç½‘é¡µã€å¾®ä¿¡ã€ç”µè¯ç­‰ï¼‰
- VIPå®¢æˆ·è¯†åˆ«ï¼šä¼˜å…ˆçº§é˜Ÿåˆ—ï¼Œå“åº”æ—¶é—´ < 30ç§’

**éåŠŸèƒ½éœ€æ±‚**ï¼š
- æ€§èƒ½ï¼šå“åº”æ—¶é—´ < 2ç§’ï¼Œå¹¶å‘ç”¨æˆ· > 1000ï¼Œååé‡ > 10000 QPS
- å¯ç”¨æ€§ï¼š99.9% ç³»ç»Ÿå¯ç”¨æ€§ï¼Œ7x24å°æ—¶æœåŠ¡ï¼Œæ•…éšœæ¢å¤ < 5åˆ†é’Ÿ
- å®‰å…¨æ€§ï¼šæ•°æ®åŠ å¯†ã€RBACæƒé™ç®¡ç†ã€å®Œæ•´å®¡è®¡æ—¥å¿—
- å¯æ‰©å±•æ€§ï¼šè‡ªåŠ¨æ‰©å®¹ã€æ’ä»¶åŒ–æ¶æ„ã€å¤šç§Ÿæˆ·æ”¯æŒ

# å®é™…é¡¹ç›®æ¶æ„å®ç°
class ProductionSystemArchitecture:
    """ç”Ÿäº§çº§ç³»ç»Ÿæ¶æ„ï¼ˆåŸºäºå®é™…é¡¹ç›®ï¼‰"""
    
    def __init__(self):
        self.core_agents = {
            "CustomerServiceAgent": {
                "file": "customer_service_system.py",
                "description": "ä¸»è¦å®¢æœæ™ºèƒ½ä½“",
                "capabilities": [
                    "å®¢æˆ·å’¨è¯¢å¤„ç†",
                    "å·¥å•ç®¡ç†",
                    "æƒ…æ„Ÿåˆ†æ",
                    "çŸ¥è¯†æ£€ç´¢",
                    "VIPå®¢æˆ·è¯†åˆ«"
                ]
            },
            "BaseAgent": {
                "file": "base_agent.py", 
                "description": "BDIæ™ºèƒ½ä½“åŸºç¡€æ¶æ„",
                "capabilities": [
                    "ä¿¡å¿µç®¡ç†",
                    "æ„¿æœ›è§„åˆ’",
                    "æ„å›¾æ‰§è¡Œ",
                    "çŠ¶æ€ç›‘æ§"
                ]
            }
        }
        
        self.infrastructure_components = {
            "æ¶ˆæ¯æ€»çº¿": {
                "file": "message_bus.py",
                "technology": "å¼‚æ­¥æ¶ˆæ¯é˜Ÿåˆ—",
                "features": ["å‘å¸ƒè®¢é˜…", "æ¶ˆæ¯æŒä¹…åŒ–", "è´Ÿè½½å‡è¡¡"]
            },
            "å·¥ä½œæµå¼•æ“": {
                "file": "langgraph_workflow.py",
                "technology": "LangGraph",
                "features": ["çŠ¶æ€ç®¡ç†", "æ¡ä»¶åˆ†æ”¯", "é”™è¯¯æ¢å¤"]
            },
            "ç›‘æ§ç³»ç»Ÿ": {
                "file": "langsmith_integration.py",
                "technology": "LangSmith",
                "features": ["é“¾è·¯è¿½è¸ª", "æ€§èƒ½ç›‘æ§", "å‘Šè­¦ç³»ç»Ÿ"]
            },
            "æ•°æ®å­˜å‚¨": {
                "components": ["PostgreSQL", "Redis", "Elasticsearch"],
                "features": ["å…³ç³»æ•°æ®", "ç¼“å­˜", "å…¨æ–‡æœç´¢"]
            }
        }
        
        self.deployment_architecture = {
            "å®¹å™¨åŒ–": {
                "technology": "Docker + Kubernetes",
                "benefits": ["ç¯å¢ƒä¸€è‡´æ€§", "å¿«é€Ÿéƒ¨ç½²", "å¼¹æ€§æ‰©å®¹"]
            },
            "å¾®æœåŠ¡": {
                "pattern": "æ¯ä¸ªæ™ºèƒ½ä½“ç‹¬ç«‹æœåŠ¡",
                "benefits": ["ç‹¬ç«‹æ‰©å±•", "æ•…éšœéš”ç¦»", "æŠ€æœ¯å¤šæ ·æ€§"]
            },
            "äº‘åŸç”Ÿ": {
                "features": ["è‡ªåŠ¨æ‰©å®¹", "æœåŠ¡å‘ç°", "é…ç½®ç®¡ç†"],
                "platforms": ["Kubernetes", "Docker Swarm", "äº‘å¹³å°"]
            }
        }

# å®é™…éƒ¨ç½²é…ç½®ç¤ºä¾‹
class ProductionDeploymentConfig:
    """ç”Ÿäº§ç¯å¢ƒéƒ¨ç½²é…ç½®"""
    
    @staticmethod
    def get_docker_compose_config():
        """Docker Compose é…ç½®"""
        return """
version: '3.8'

services:
  # ä¸»åº”ç”¨æœåŠ¡
  multi-agent-system:
    build: .
    ports:
      - "8000:8000"
    environment:
      - ENVIRONMENT=production
      - DATABASE_URL=postgresql://user:pass@postgres:5432/multiagent
      - REDIS_URL=redis://redis:6379
      - LANGSMITH_API_KEY=${LANGSMITH_API_KEY}
    depends_on:
      - postgres
      - redis
      - elasticsearch
    volumes:
      - ./logs:/app/logs
      - ./data:/app/data
    restart: unless-stopped
    
  # æ•°æ®åº“æœåŠ¡
  postgres:
    image: postgres:15
    environment:
      - POSTGRES_DB=multiagent
      - POSTGRES_USER=user
      - POSTGRES_PASSWORD=pass
    volumes:
      - postgres_data:/var/lib/postgresql/data
    ports:
      - "5432:5432"
    restart: unless-stopped
    
  # ç¼“å­˜æœåŠ¡
  redis:
    image: redis:7-alpine
    ports:
      - "6379:6379"
    volumes:
      - redis_data:/data
    restart: unless-stopped
    
  # æœç´¢å¼•æ“
  elasticsearch:
    image: elasticsearch:8.8.0
    environment:
      - discovery.type=single-node
      - xpack.security.enabled=false
    ports:
      - "9200:9200"
    volumes:
      - es_data:/usr/share/elasticsearch/data
    restart: unless-stopped
    
  # ç›‘æ§æœåŠ¡
  prometheus:
    image: prom/prometheus
    ports:
      - "9090:9090"
    volumes:
      - ./monitoring/prometheus.yml:/etc/prometheus/prometheus.yml
      - prometheus_data:/prometheus
    restart: unless-stopped
    
  # å¯è§†åŒ–é¢æ¿
  grafana:
    image: grafana/grafana
    ports:
      - "3000:3000"
    environment:
      - GF_SECURITY_ADMIN_PASSWORD=admin
    volumes:
      - grafana_data:/var/lib/grafana
      - ./monitoring/grafana:/etc/grafana/provisioning
    restart: unless-stopped

volumes:
  postgres_data:
  redis_data:
  es_data:
  prometheus_data:
  grafana_data:
"""
    
    @staticmethod
    def get_kubernetes_config():
        """Kubernetes éƒ¨ç½²é…ç½®"""
        return """
apiVersion: apps/v1
kind: Deployment
metadata:
  name: multi-agent-system
  labels:
    app: multi-agent-system
spec:
  replicas: 3
  selector:
    matchLabels:
      app: multi-agent-system
  template:
    metadata:
      labels:
        app: multi-agent-system
    spec:
      containers:
      - name: multi-agent-system
        image: multi-agent-system:latest
        ports:
        - containerPort: 8000
        env:
        - name: ENVIRONMENT
          value: "production"
        - name: DATABASE_URL
          valueFrom:
            secretKeyRef:
              name: db-secret
              key: url
        - name: REDIS_URL
          value: "redis://redis-service:6379"
        - name: LANGSMITH_API_KEY
          valueFrom:
            secretKeyRef:
              name: langsmith-secret
              key: api-key
        resources:
          requests:
            memory: "512Mi"
            cpu: "250m"
          limits:
            memory: "1Gi"
            cpu: "500m"
        livenessProbe:
          httpGet:
            path: /health
            port: 8000
          initialDelaySeconds: 30
          periodSeconds: 10
        readinessProbe:
          httpGet:
            path: /ready
            port: 8000
          initialDelaySeconds: 5
          periodSeconds: 5
---
apiVersion: v1
kind: Service
metadata:
  name: multi-agent-system-service
spec:
  selector:
    app: multi-agent-system
  ports:
  - protocol: TCP
    port: 80
    targetPort: 8000
  type: LoadBalancer
---
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: multi-agent-system-hpa
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: multi-agent-system
  minReplicas: 3
  maxReplicas: 10
  metrics:
  - type: Resource
    resource:
      name: cpu
      target:
        type: Utilization
        averageUtilization: 70
  - type: Resource
    resource:
      name: memory
      target:
        type: Utilization
        averageUtilization: 80
"""
```

### 1.2 ç”Ÿäº§ç¯å¢ƒéƒ¨ç½²å®è·µ

#### 1.2.1 å®¹å™¨åŒ–éƒ¨ç½²

```python
# åŸºäºå®é™…é¡¹ç›®çš„ Dockerfile
class ProductionDockerfile:
    """ç”Ÿäº§çº§ Dockerfile é…ç½®"""
    
    @staticmethod
    def get_dockerfile():
        return """
# å¤šé˜¶æ®µæ„å»ºï¼Œä¼˜åŒ–é•œåƒå¤§å°
FROM python:3.11-slim as builder

# è®¾ç½®å·¥ä½œç›®å½•
WORKDIR /app

# å®‰è£…ç³»ç»Ÿä¾èµ–
RUN apt-get update && apt-get install -y \\
    gcc \\
    g++ \\
    && rm -rf /var/lib/apt/lists/*

# å¤åˆ¶ä¾èµ–æ–‡ä»¶
COPY requirements.txt .

# å®‰è£…Pythonä¾èµ–
RUN pip install --no-cache-dir --user -r requirements.txt

# ç”Ÿäº§é˜¶æ®µ
FROM python:3.11-slim

# åˆ›å»ºérootç”¨æˆ·
RUN useradd --create-home --shell /bin/bash app

# è®¾ç½®å·¥ä½œç›®å½•
WORKDIR /app

# ä»builderé˜¶æ®µå¤åˆ¶ä¾èµ–
COPY --from=builder /root/.local /home/app/.local

# å¤åˆ¶åº”ç”¨ä»£ç 
COPY . .

# è®¾ç½®æƒé™
RUN chown -R app:app /app

# åˆ‡æ¢åˆ°érootç”¨æˆ·
USER app

# è®¾ç½®ç¯å¢ƒå˜é‡
ENV PATH=/home/app/.local/bin:$PATH
ENV PYTHONPATH=/app

# å¥åº·æ£€æŸ¥
HEALTHCHECK --interval=30s --timeout=30s --start-period=5s --retries=3 \\
    CMD python -c "import requests; requests.get('http://localhost:8000/health')"

# æš´éœ²ç«¯å£
EXPOSE 8000

# å¯åŠ¨å‘½ä»¤
CMD ["python", "main.py"]
"""

# ç”Ÿäº§ç¯å¢ƒå¯åŠ¨è„šæœ¬
class ProductionStartupScript:
    """ç”Ÿäº§ç¯å¢ƒå¯åŠ¨è„šæœ¬"""
    
    @staticmethod
    def get_startup_script():
        return """
#!/bin/bash

# åŸºäºé¡¹ç›® multi_agent_system/main.py çš„å¯åŠ¨æµç¨‹

set -e

echo "Starting Multi-Agent System in Production Mode..."

# ç¯å¢ƒæ£€æŸ¥
echo "Checking environment..."
python -c "
import sys
import os
from src.agents.base_agent import BaseAgent
from src.applications.customer_service_system import CustomerServiceAgent
from src.communication.message_bus import MessageBus
from src.workflows.langgraph_workflow import StateGraph
from src.monitoring.langsmith_integration import EnterpriseTracing

print('âœ“ All modules imported successfully')
print(f'âœ“ Python version: {sys.version}')
print(f'âœ“ Environment: {os.getenv(\"ENVIRONMENT\", \"development\")}')
"

# æ•°æ®åº“è¿ç§»
echo "Running database migrations..."
python -c "
from src.database.migrations import run_migrations
run_migrations()
print('âœ“ Database migrations completed')
"

# é¢„çƒ­ç³»ç»Ÿ
echo "Warming up system..."
python -c "
import asyncio
from main import MultiAgentSystem

async def warmup():
    system = MultiAgentSystem()
    await system.initialize()
    print('âœ“ System warmup completed')
    await system.shutdown()

asyncio.run(warmup())
"

# å¯åŠ¨ä¸»åº”ç”¨
echo "Starting main application..."
exec python main.py
"""
```

### 1.2 æ ¸å¿ƒæ™ºèƒ½ä½“å®ç°

#### 1.2.1 å¯¹è¯ç®¡ç†æ™ºèƒ½ä½“

```python
from langgraph import StateGraph, END
from langchain.schema import BaseMessage
from typing import Dict, List, Any
import asyncio

class DialogManagerAgent:
    """å¯¹è¯ç®¡ç†æ™ºèƒ½ä½“"""
    
    def __init__(self):
        self.conversation_history = {}
        self.context_window = 10
        
    async def process_message(self, user_id: str, message: str) -> Dict[str, Any]:
        """å¤„ç†ç”¨æˆ·æ¶ˆæ¯"""
        # è·å–å¯¹è¯å†å²
        history = self.get_conversation_history(user_id)
        
        # æ„å»ºçŠ¶æ€å›¾
        workflow = StateGraph(ConversationState)
        
        # æ·»åŠ èŠ‚ç‚¹
        workflow.add_node("intent_recognition", self.recognize_intent)
        workflow.add_node("knowledge_retrieval", self.retrieve_knowledge)
        workflow.add_node("response_generation", self.generate_response)
        workflow.add_node("sentiment_analysis", self.analyze_sentiment)
        
        # å®šä¹‰è¾¹
        workflow.add_edge("intent_recognition", "knowledge_retrieval")
        workflow.add_edge("knowledge_retrieval", "response_generation")
        workflow.add_edge("response_generation", "sentiment_analysis")
        workflow.add_edge("sentiment_analysis", END)
        
        # è®¾ç½®å…¥å£ç‚¹
        workflow.set_entry_point("intent_recognition")
        
        # ç¼–è¯‘å¹¶è¿è¡Œ
        app = workflow.compile()
        
        initial_state = ConversationState(
            user_id=user_id,
            message=message,
            history=history,
            context={}
        )
        
        result = await app.ainvoke(initial_state)
        return result

class ConversationState:
    """å¯¹è¯çŠ¶æ€"""
    def __init__(self, user_id: str, message: str, history: List, context: Dict):
        self.user_id = user_id
        self.message = message
        self.history = history
        self.context = context
        self.intent = None
        self.knowledge = None
        self.response = None
        self.sentiment = None
```

#### 1.2.2 çŸ¥è¯†æ£€ç´¢æ™ºèƒ½ä½“

```python
from langchain.vectorstores import Chroma
from langchain.embeddings import OpenAIEmbeddings
from langchain.text_splitter import RecursiveCharacterTextSplitter

class KnowledgeRetrieverAgent:
    """çŸ¥è¯†æ£€ç´¢æ™ºèƒ½ä½“"""
    
    def __init__(self):
        self.embeddings = OpenAIEmbeddings()
        self.vectorstore = None
        self.setup_knowledge_base()
        
    def setup_knowledge_base(self):
        """è®¾ç½®çŸ¥è¯†åº“"""
        # åŠ è½½ä¼ä¸šçŸ¥è¯†æ–‡æ¡£
        documents = self.load_knowledge_documents()
        
        # æ–‡æ¡£åˆ†å‰²
        text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=1000,
            chunk_overlap=200
        )
        splits = text_splitter.split_documents(documents)
        
        # åˆ›å»ºå‘é‡å­˜å‚¨
        self.vectorstore = Chroma.from_documents(
            documents=splits,
            embedding=self.embeddings,
            persist_directory="./knowledge_db"
        )
    
    async def retrieve_knowledge(self, query: str, top_k: int = 5) -> List[Dict]:
        """æ£€ç´¢ç›¸å…³çŸ¥è¯†"""
        # ç›¸ä¼¼æ€§æœç´¢
        docs = self.vectorstore.similarity_search_with_score(query, k=top_k)
        
        # é‡æ’åº
        reranked_docs = await self.rerank_documents(query, docs)
        
        return [
            {
                "content": doc.page_content,
                "metadata": doc.metadata,
                "score": score
            }
            for doc, score in reranked_docs
        ]
    
    async def rerank_documents(self, query: str, docs: List) -> List:
        """æ–‡æ¡£é‡æ’åº"""
        # ä½¿ç”¨æ›´ç²¾ç¡®çš„é‡æ’åºæ¨¡å‹
        # è¿™é‡Œå¯ä»¥é›†æˆ Cohere Rerank æˆ–å…¶ä»–é‡æ’åºæœåŠ¡
        return docs  # ç®€åŒ–å®ç°
```

### 1.3 ç³»ç»Ÿé›†æˆä¸æµ‹è¯•

#### 1.3.1 é›†æˆæµ‹è¯•æ¡†æ¶

```python
import pytest
import asyncio
from unittest.mock import Mock, patch

class IntegrationTestSuite:
    """é›†æˆæµ‹è¯•å¥—ä»¶"""
    
    @pytest.fixture
    async def customer_service_system(self):
        """å®¢æœç³»ç»Ÿæµ‹è¯•å¤¹å…·"""
        system = CustomerServiceSystem()
        await system.initialize()
        yield system
        await system.cleanup()
    
    @pytest.mark.asyncio
    async def test_end_to_end_conversation(self, customer_service_system):
        """ç«¯åˆ°ç«¯å¯¹è¯æµ‹è¯•"""
        user_id = "test_user_001"
        
        # æµ‹è¯•åœºæ™¯1ï¼šäº§å“å’¨è¯¢
        response1 = await customer_service_system.process_message(
            user_id, "æˆ‘æƒ³äº†è§£ä½ ä»¬çš„äº§å“ä»·æ ¼"
        )
        assert response1["intent"] == "product_inquiry"
        assert "ä»·æ ¼" in response1["response"]
        
        # æµ‹è¯•åœºæ™¯2ï¼šæŠ€æœ¯æ”¯æŒ
        response2 = await customer_service_system.process_message(
            user_id, "æˆ‘çš„è´¦å·ç™»å½•ä¸äº†"
        )
        assert response2["intent"] == "technical_support"
        assert response2["escalate_to_human"] == False
        
        # æµ‹è¯•åœºæ™¯3ï¼šæŠ•è¯‰å¤„ç†
        response3 = await customer_service_system.process_message(
            user_id, "ä½ ä»¬çš„æœåŠ¡å¤ªå·®äº†ï¼Œæˆ‘è¦æŠ•è¯‰"
        )
        assert response3["sentiment"] == "negative"
        assert response3["escalate_to_human"] == True
    
    @pytest.mark.asyncio
    async def test_performance_under_load(self, customer_service_system):
        """è´Ÿè½½æµ‹è¯•"""
        # æ¨¡æ‹Ÿ100ä¸ªå¹¶å‘ç”¨æˆ·
        tasks = []
        for i in range(100):
            task = customer_service_system.process_message(
                f"user_{i}", "Hello, I need help"
            )
            tasks.append(task)
        
        # æ‰§è¡Œå¹¶å‘æµ‹è¯•
        start_time = asyncio.get_event_loop().time()
        results = await asyncio.gather(*tasks)
        end_time = asyncio.get_event_loop().time()
        
        # éªŒè¯æ€§èƒ½æŒ‡æ ‡
        avg_response_time = (end_time - start_time) / len(results)
        assert avg_response_time < 2.0  # å¹³å‡å“åº”æ—¶é—´å°äº2ç§’
        assert all(result["status"] == "success" for result in results)
```

## 2. ç”Ÿäº§ç¯å¢ƒéƒ¨ç½²

### 2.1 å®¹å™¨åŒ–éƒ¨ç½²

#### 2.1.1 Dockeré…ç½®

```dockerfile
# Dockerfile
FROM python:3.11-slim

WORKDIR /app

# å®‰è£…ç³»ç»Ÿä¾èµ–
RUN apt-get update && apt-get install -y \
    gcc \
    g++ \
    && rm -rf /var/lib/apt/lists/*

# å¤åˆ¶ä¾èµ–æ–‡ä»¶
COPY requirements.txt .
RUN pip install --no-cache-dir -r requirements.txt

# å¤åˆ¶åº”ç”¨ä»£ç 
COPY . .

# è®¾ç½®ç¯å¢ƒå˜é‡
ENV PYTHONPATH=/app
ENV LANGCHAIN_TRACING_V2=true
ENV LANGCHAIN_ENDPOINT=https://api.smith.langchain.com

# æš´éœ²ç«¯å£
EXPOSE 8000

# å¥åº·æ£€æŸ¥
HEALTHCHECK --interval=30s --timeout=30s --start-period=5s --retries=3 \
    CMD curl -f http://localhost:8000/health || exit 1

# å¯åŠ¨å‘½ä»¤
CMD ["uvicorn", "main:app", "--host", "0.0.0.0", "--port", "8000"]
```

#### 2.1.2 Docker Composeé…ç½®

```yaml
# docker-compose.yml
version: '3.8'

services:
  customer-service:
    build: .
    ports:
      - "8000:8000"
    environment:
      - DATABASE_URL=postgresql://user:password@postgres:5432/customer_service
      - REDIS_URL=redis://redis:6379
      - ELASTICSEARCH_URL=http://elasticsearch:9200
    depends_on:
      - postgres
      - redis
      - elasticsearch
    volumes:
      - ./logs:/app/logs
    restart: unless-stopped

  postgres:
    image: postgres:15
    environment:
      POSTGRES_DB: customer_service
      POSTGRES_USER: user
      POSTGRES_PASSWORD: password
    volumes:
      - postgres_data:/var/lib/postgresql/data
    ports:
      - "5432:5432"

  redis:
    image: redis:7-alpine
    ports:
      - "6379:6379"
    volumes:
      - redis_data:/data

  elasticsearch:
    image: elasticsearch:8.8.0
    environment:
      - discovery.type=single-node
      - xpack.security.enabled=false
    ports:
      - "9200:9200"
    volumes:
      - elasticsearch_data:/usr/share/elasticsearch/data

  nginx:
    image: nginx:alpine
    ports:
      - "80:80"
      - "443:443"
    volumes:
      - ./nginx.conf:/etc/nginx/nginx.conf
      - ./ssl:/etc/nginx/ssl
    depends_on:
      - customer-service

volumes:
  postgres_data:
  redis_data:
  elasticsearch_data:
```

### 2.2 Kuberneteséƒ¨ç½²

#### 2.2.1 K8séƒ¨ç½²é…ç½®

```yaml
# k8s-deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: customer-service
  labels:
    app: customer-service
spec:
  replicas: 3
  selector:
    matchLabels:
      app: customer-service
  template:
    metadata:
      labels:
        app: customer-service
    spec:
      containers:
      - name: customer-service
        image: customer-service:latest
        ports:
        - containerPort: 8000
        env:
        - name: DATABASE_URL
          valueFrom:
            secretKeyRef:
              name: db-secret
              key: url
        - name: REDIS_URL
          value: "redis://redis-service:6379"
        resources:
          requests:
            memory: "512Mi"
            cpu: "250m"
          limits:
            memory: "1Gi"
            cpu: "500m"
        livenessProbe:
          httpGet:
            path: /health
            port: 8000
          initialDelaySeconds: 30
          periodSeconds: 10
        readinessProbe:
          httpGet:
            path: /ready
            port: 8000
          initialDelaySeconds: 5
          periodSeconds: 5

---
apiVersion: v1
kind: Service
metadata:
  name: customer-service
spec:
  selector:
    app: customer-service
  ports:
  - port: 80
    targetPort: 8000
  type: LoadBalancer

---
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: customer-service-hpa
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: customer-service
  minReplicas: 3
  maxReplicas: 10
  metrics:
  - type: Resource
    resource:
      name: cpu
      target:
        type: Utilization
        averageUtilization: 70
  - type: Resource
    resource:
      name: memory
      target:
        type: Utilization
        averageUtilization: 80
```

## 3. ä¼ä¸šçº§ç›‘æ§ä¸è¿ç»´

### 3.1 åŸºäºå®é™…é¡¹ç›®çš„ç›‘æ§ç³»ç»Ÿ

#### 3.1.1 ä¼ä¸šçº§ç›‘æ§æ¶æ„

```python
# åŸºäºé¡¹ç›® multi_agent_system/src/monitoring/langsmith_integration.py çš„ç›‘æ§å®ç°
import asyncio
import time
import psutil
import logging
from typing import Dict, List, Any, Optional
from dataclasses import dataclass, asdict
from datetime import datetime, timedelta
from prometheus_client import Counter, Histogram, Gauge, start_http_server

@dataclass
class SystemMetrics:
    """ç³»ç»ŸæŒ‡æ ‡"""
    timestamp: datetime
    cpu_usage: float
    memory_usage: float
    disk_usage: float
    network_io: Dict[str, int]
    active_connections: int
    response_time_p95: float
    error_rate: float
    throughput: float

@dataclass
class AgentMetrics:
    """æ™ºèƒ½ä½“æŒ‡æ ‡"""
    agent_id: str
    agent_type: str
    total_requests: int
    successful_requests: int
    failed_requests: int
    avg_response_time: float
    current_load: float
    memory_usage: float
    last_activity: datetime

class EnterpriseMonitoringSystem:
    """ä¼ä¸šçº§ç›‘æ§ç³»ç»Ÿï¼ˆåŸºäºå®é™…é¡¹ç›®ï¼‰"""
    
    def __init__(self):
        self.setup_prometheus_metrics()
        self.setup_logging()
        self.alert_manager = AlertManager()
        self.performance_analyzer = PerformanceAnalyzer()
        
    def setup_prometheus_metrics(self):
        """è®¾ç½® Prometheus æŒ‡æ ‡"""
        # è¯·æ±‚æŒ‡æ ‡
        self.request_counter = Counter(
            'multiagent_requests_total',
            'Total number of requests',
            ['agent_type', 'status']
        )
        
        self.request_duration = Histogram(
            'multiagent_request_duration_seconds',
            'Request duration in seconds',
            ['agent_type', 'endpoint']
        )
        
        # ç³»ç»ŸæŒ‡æ ‡
        self.cpu_usage = Gauge(
            'multiagent_cpu_usage_percent',
            'CPU usage percentage'
        )
        
        self.memory_usage = Gauge(
            'multiagent_memory_usage_bytes',
            'Memory usage in bytes'
        )
        
        self.active_agents = Gauge(
            'multiagent_active_agents',
            'Number of active agents',
            ['agent_type']
        )
        
        # ä¸šåŠ¡æŒ‡æ ‡
        self.customer_satisfaction = Gauge(
            'multiagent_customer_satisfaction_score',
            'Customer satisfaction score'
        )
        
        self.ticket_resolution_time = Histogram(
            'multiagent_ticket_resolution_seconds',
            'Ticket resolution time in seconds',
            ['priority', 'category']
        )
    
    def setup_logging(self):
        """è®¾ç½®æ—¥å¿—ç³»ç»Ÿ"""
        logging.basicConfig(
            level=logging.INFO,
            format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
            handlers=[
                logging.FileHandler('/app/logs/multiagent.log'),
                logging.StreamHandler()
            ]
        )
        self.logger = logging.getLogger(__name__)
    
    async def collect_system_metrics(self) -> SystemMetrics:
        """æ”¶é›†ç³»ç»ŸæŒ‡æ ‡"""
        # CPU ä½¿ç”¨ç‡
        cpu_usage = psutil.cpu_percent(interval=1)
        
        # å†…å­˜ä½¿ç”¨ç‡
        memory = psutil.virtual_memory()
        memory_usage = memory.percent
        
        # ç£ç›˜ä½¿ç”¨ç‡
        disk = psutil.disk_usage('/')
        disk_usage = disk.percent
        
        # ç½‘ç»œIO
        network = psutil.net_io_counters()
        network_io = {
            'bytes_sent': network.bytes_sent,
            'bytes_recv': network.bytes_recv
        }
        
        # æ´»è·ƒè¿æ¥æ•°
        connections = len(psutil.net_connections())
        
        # æ›´æ–° Prometheus æŒ‡æ ‡
        self.cpu_usage.set(cpu_usage)
        self.memory_usage.set(memory.used)
        
        return SystemMetrics(
            timestamp=datetime.now(),
            cpu_usage=cpu_usage,
            memory_usage=memory_usage,
            disk_usage=disk_usage,
            network_io=network_io,
            active_connections=connections,
            response_time_p95=0.0,  # ä»åº”ç”¨æŒ‡æ ‡è·å–
            error_rate=0.0,         # ä»åº”ç”¨æŒ‡æ ‡è·å–
            throughput=0.0          # ä»åº”ç”¨æŒ‡æ ‡è·å–
        )
    
    async def monitor_agent_performance(self, agent_id: str) -> AgentMetrics:
        """ç›‘æ§æ™ºèƒ½ä½“æ€§èƒ½"""
        # ä»å®é™…é¡¹ç›®çš„æ™ºèƒ½ä½“è·å–æŒ‡æ ‡
        agent_stats = await self.get_agent_statistics(agent_id)
        
        metrics = AgentMetrics(
            agent_id=agent_id,
            agent_type=agent_stats.get('type', 'unknown'),
            total_requests=agent_stats.get('total_requests', 0),
            successful_requests=agent_stats.get('successful_requests', 0),
            failed_requests=agent_stats.get('failed_requests', 0),
            avg_response_time=agent_stats.get('avg_response_time', 0.0),
            current_load=agent_stats.get('current_load', 0.0),
            memory_usage=agent_stats.get('memory_usage', 0.0),
            last_activity=datetime.now()
        )
        
        # æ›´æ–° Prometheus æŒ‡æ ‡
        self.active_agents.labels(agent_type=metrics.agent_type).set(1)
        
        return metrics

class AlertManager:
    """å‘Šè­¦ç®¡ç†å™¨"""
    
    def __init__(self):
        self.alert_rules = self.load_alert_rules()
        self.notification_channels = self.setup_notification_channels()
    
    def load_alert_rules(self) -> List[Dict]:
        """åŠ è½½å‘Šè­¦è§„åˆ™"""
        return [
            {
                "name": "high_cpu_usage",
                "condition": "cpu_usage > 80",
                "severity": "warning",
                "duration": "5m",
                "message": "CPU usage is above 80% for 5 minutes"
            },
            {
                "name": "high_memory_usage", 
                "condition": "memory_usage > 90",
                "severity": "critical",
                "duration": "2m",
                "message": "Memory usage is above 90% for 2 minutes"
            },
            {
                "name": "high_error_rate",
                "condition": "error_rate > 5",
                "severity": "critical", 
                "duration": "1m",
                "message": "Error rate is above 5% for 1 minute"
            },
            {
                "name": "slow_response_time",
                "condition": "response_time_p95 > 5",
                "severity": "warning",
                "duration": "3m", 
                "message": "95th percentile response time is above 5 seconds"
            },
            {
                "name": "agent_down",
                "condition": "active_agents == 0",
                "severity": "critical",
                "duration": "30s",
                "message": "No active agents detected"
            }
        ]

class PerformanceAnalyzer:
    """æ€§èƒ½åˆ†æå™¨"""
    
    def __init__(self):
        self.metrics_history = []
        self.analysis_window = timedelta(hours=1)
    
    async def analyze_performance_trends(self, metrics: SystemMetrics) -> Dict[str, Any]:
        """åˆ†ææ€§èƒ½è¶‹åŠ¿"""
        self.metrics_history.append(metrics)
        
        # ä¿æŒåˆ†æçª—å£å¤§å°
        cutoff_time = datetime.now() - self.analysis_window
        self.metrics_history = [
            m for m in self.metrics_history 
            if m.timestamp > cutoff_time
        ]
        
        if len(self.metrics_history) < 10:
            return {"status": "insufficient_data"}
        
        # è®¡ç®—è¶‹åŠ¿
        cpu_trend = self.calculate_trend([m.cpu_usage for m in self.metrics_history])
        memory_trend = self.calculate_trend([m.memory_usage for m in self.metrics_history])
        response_time_trend = self.calculate_trend([m.response_time_p95 for m in self.metrics_history])
        
        # å¼‚å¸¸æ£€æµ‹
        anomalies = self.detect_anomalies()
        
        # æ€§èƒ½å»ºè®®
        recommendations = self.generate_recommendations(cpu_trend, memory_trend, response_time_trend)
        
        return {
            "status": "analysis_complete",
            "trends": {
                "cpu": cpu_trend,
                "memory": memory_trend,
                "response_time": response_time_trend
            },
            "anomalies": anomalies,
            "recommendations": recommendations,
            "analysis_period": {
                "start": self.metrics_history[0].timestamp.isoformat(),
                "end": self.metrics_history[-1].timestamp.isoformat(),
                "sample_count": len(self.metrics_history)
            }
        }

# ç”Ÿäº§ç¯å¢ƒç›‘æ§é…ç½®
class ProductionMonitoringConfig:
    """ç”Ÿäº§ç¯å¢ƒç›‘æ§é…ç½®"""
    
    @staticmethod
    def get_prometheus_config():
        """Prometheus é…ç½®"""
        return """
global:
  scrape_interval: 15s
  evaluation_interval: 15s

rule_files:
  - "alert_rules.yml"

alerting:
  alertmanagers:
    - static_configs:
        - targets:
          - alertmanager:9093

scrape_configs:
  - job_name: 'multi-agent-system'
    static_configs:
      - targets: ['multi-agent-system:8000']
    metrics_path: '/metrics'
    scrape_interval: 10s
    
  - job_name: 'node-exporter'
    static_configs:
      - targets: ['node-exporter:9100']
    
  - job_name: 'postgres-exporter'
    static_configs:
      - targets: ['postgres-exporter:9187']
    
  - job_name: 'redis-exporter'
    static_configs:
      - targets: ['redis-exporter:9121']
"""
    
    @staticmethod
    def get_grafana_dashboard():
        """Grafana ä»ªè¡¨æ¿é…ç½®"""
        return {
            "dashboard": {
                "title": "Multi-Agent System Monitoring",
                "panels": [
                    {
                        "title": "System Overview",
                        "type": "stat",
                        "targets": [
                            {"expr": "multiagent_cpu_usage_percent"},
                            {"expr": "multiagent_memory_usage_bytes / 1024 / 1024 / 1024"},
                            {"expr": "rate(multiagent_requests_total[5m])"}
                        ]
                    },
                    {
                        "title": "Request Rate",
                        "type": "graph",
                        "targets": [
                            {"expr": "rate(multiagent_requests_total[5m])"}
                        ]
                    },
                    {
                        "title": "Response Time",
                        "type": "graph", 
                        "targets": [
                            {"expr": "histogram_quantile(0.95, multiagent_request_duration_seconds_bucket)"},
                            {"expr": "histogram_quantile(0.50, multiagent_request_duration_seconds_bucket)"}
                        ]
                    },
                    {
                        "title": "Error Rate",
                        "type": "graph",
                        "targets": [
                            {"expr": "rate(multiagent_requests_total{status=\"error\"}[5m]) / rate(multiagent_requests_total[5m]) * 100"}
                        ]
                    },
                    {
                        "title": "Active Agents",
                        "type": "stat",
                        "targets": [
                            {"expr": "multiagent_active_agents"}
                        ]
                    },
                    {
                        "title": "Customer Satisfaction",
                        "type": "gauge",
                        "targets": [
                            {"expr": "multiagent_customer_satisfaction_score"}
                        ]
                    }
                ]
            }
        }

# å¯åŠ¨ç›‘æ§ç³»ç»Ÿ
async def start_monitoring_system():
    """å¯åŠ¨ç›‘æ§ç³»ç»Ÿ"""
    monitoring = EnterpriseMonitoringSystem()
    
    # å¯åŠ¨ Prometheus æŒ‡æ ‡æœåŠ¡å™¨
    start_http_server(8001)
    
    # ç›‘æ§å¾ªç¯
    while True:
        try:
            # æ”¶é›†ç³»ç»ŸæŒ‡æ ‡
            system_metrics = await monitoring.collect_system_metrics()
            
            # æ£€æŸ¥å‘Šè­¦
            await monitoring.alert_manager.check_alerts(system_metrics)
            
            # æ€§èƒ½åˆ†æ
            analysis = await monitoring.performance_analyzer.analyze_performance_trends(system_metrics)
            
            # è®°å½•æ—¥å¿—
            monitoring.logger.info(f"System metrics collected: {system_metrics}")
            
            if analysis.get("status") == "analysis_complete":
                monitoring.logger.info(f"Performance analysis: {analysis}")
            
            # ç­‰å¾…ä¸‹ä¸€æ¬¡æ”¶é›†
            await asyncio.sleep(30)
            
        except Exception as e:
            monitoring.logger.error(f"Monitoring error: {e}")
            await asyncio.sleep(60)
```

### 3.2 è‡ªåŠ¨åŒ–è¿ç»´å®è·µ

#### 3.2.1 è‡ªåŠ¨æ‰©ç¼©å®¹

```python
# åŸºäºå®é™…é¡¹ç›®çš„è‡ªåŠ¨æ‰©ç¼©å®¹å®ç°
import kubernetes
from kubernetes import client, config
import asyncio
import logging
from typing import Dict, List, Any
from dataclasses import dataclass
from datetime import datetime, timedelta

@dataclass
class ScalingMetrics:
    """æ‰©ç¼©å®¹æŒ‡æ ‡"""
    cpu_utilization: float
    memory_utilization: float
    request_rate: float
    response_time_p95: float
    error_rate: float
    queue_length: int
    active_connections: int

@dataclass
class ScalingDecision:
    """æ‰©ç¼©å®¹å†³ç­–"""
    action: str  # "scale_up", "scale_down", "no_action"
    target_replicas: int
    reason: str
    confidence: float
    timestamp: datetime

class AutoScaler:
    """è‡ªåŠ¨æ‰©ç¼©å®¹å™¨"""
    
    def __init__(self):
        self.setup_kubernetes_client()
        self.scaling_config = self.load_scaling_config()
        self.metrics_history = []
        self.logger = logging.getLogger(__name__)
    
    def setup_kubernetes_client(self):
        """è®¾ç½® Kubernetes å®¢æˆ·ç«¯"""
        try:
            # å°è¯•é›†ç¾¤å†…é…ç½®
            config.load_incluster_config()
        except:
            # ä½¿ç”¨æœ¬åœ°é…ç½®
            config.load_kube_config()
        
        self.apps_v1 = client.AppsV1Api()
        self.core_v1 = client.CoreV1Api()
    
    def load_scaling_config(self) -> Dict[str, Any]:
        """åŠ è½½æ‰©ç¼©å®¹é…ç½®"""
        return {
            "deployment_name": "multi-agent-system",
            "namespace": "default",
            "min_replicas": 2,
            "max_replicas": 20,
            "target_cpu_utilization": 70,
            "target_memory_utilization": 80,
            "target_response_time": 2.0,
            "scale_up_threshold": {
                "cpu": 80,
                "memory": 85,
                "response_time": 3.0,
                "error_rate": 5.0
            },
            "scale_down_threshold": {
                "cpu": 30,
                "memory": 40,
                "response_time": 1.0,
                "error_rate": 1.0
            },
            "cooldown_period": 300,  # 5åˆ†é’Ÿå†·å´æœŸ
            "scale_up_factor": 1.5,
            "scale_down_factor": 0.8
        }
    
    async def make_scaling_decision(self, metrics: ScalingMetrics) -> ScalingDecision:
        """åˆ¶å®šæ‰©ç¼©å®¹å†³ç­–"""
        current_replicas = await self.get_current_replicas()
        
        # æ£€æŸ¥å†·å´æœŸ
        if not self.is_cooldown_expired():
            return ScalingDecision(
                action="no_action",
                target_replicas=current_replicas,
                reason="åœ¨å†·å´æœŸå†…",
                confidence=1.0,
                timestamp=datetime.now()
            )
        
        # æ‰©å®¹æ¡ä»¶æ£€æŸ¥
        scale_up_score = self.calculate_scale_up_score(metrics)
        scale_down_score = self.calculate_scale_down_score(metrics)
        
        if scale_up_score > 0.7:
            target_replicas = min(
                int(current_replicas * self.scaling_config["scale_up_factor"]),
                self.scaling_config["max_replicas"]
            )
            return ScalingDecision(
                action="scale_up",
                target_replicas=target_replicas,
                reason=f"æ‰©å®¹è¯„åˆ†: {scale_up_score:.2f}",
                confidence=scale_up_score,
                timestamp=datetime.now()
            )
        
        elif scale_down_score > 0.7:
            target_replicas = max(
                int(current_replicas * self.scaling_config["scale_down_factor"]),
                self.scaling_config["min_replicas"]
            )
            return ScalingDecision(
                action="scale_down",
                target_replicas=target_replicas,
                reason=f"ç¼©å®¹è¯„åˆ†: {scale_down_score:.2f}",
                confidence=scale_down_score,
                timestamp=datetime.now()
            )
        
        return ScalingDecision(
            action="no_action",
            target_replicas=current_replicas,
            reason="æŒ‡æ ‡æ­£å¸¸",
            confidence=0.5,
            timestamp=datetime.now()
        )

# è‡ªåŠ¨åŒ–è¿ç»´ä¸»å¾ªç¯
async def auto_scaling_loop():
    """è‡ªåŠ¨æ‰©ç¼©å®¹ä¸»å¾ªç¯"""
    autoscaler = AutoScaler()
    
    while True:
        try:
            # æ”¶é›†æŒ‡æ ‡
            metrics = await autoscaler.collect_scaling_metrics()
            
            # åˆ¶å®šå†³ç­–
            decision = await autoscaler.make_scaling_decision(metrics)
            
            # æ‰§è¡Œæ‰©ç¼©å®¹
            success = await autoscaler.execute_scaling(decision)
            
            if success:
                autoscaler.logger.info(f"æ‰©ç¼©å®¹å†³ç­–: {decision}")
            
            # ç­‰å¾…ä¸‹ä¸€æ¬¡æ£€æŸ¥
            await asyncio.sleep(60)  # æ¯åˆ†é’Ÿæ£€æŸ¥ä¸€æ¬¡
            
        except Exception as e:
            autoscaler.logger.error(f"è‡ªåŠ¨æ‰©ç¼©å®¹é”™è¯¯: {e}")
            await asyncio.sleep(300)  # é”™è¯¯æ—¶ç­‰å¾…5åˆ†é’Ÿ
```

### 3.3 æ•…éšœæ¢å¤ä¸ç¾å¤‡

#### 3.3.1 æ•…éšœæ£€æµ‹ä¸è‡ªåŠ¨æ¢å¤

```python
# åŸºäºå®é™…é¡¹ç›®çš„æ•…éšœæ¢å¤æœºåˆ¶
import asyncio
import logging
from typing import Dict, List, Any, Optional
from dataclasses import dataclass
from datetime import datetime, timedelta
from enum import Enum

class FailureType(Enum):
    """æ•…éšœç±»å‹"""
    SERVICE_DOWN = "service_down"
    HIGH_ERROR_RATE = "high_error_rate"
    SLOW_RESPONSE = "slow_response"
    MEMORY_LEAK = "memory_leak"
    DATABASE_CONNECTION = "database_connection"
    NETWORK_ISSUE = "network_issue"

class RecoveryAction(Enum):
    """æ¢å¤åŠ¨ä½œ"""
    RESTART_SERVICE = "restart_service"
    SCALE_UP = "scale_up"
    FAILOVER = "failover"
    CIRCUIT_BREAKER = "circuit_breaker"
    ROLLBACK = "rollback"
    MANUAL_INTERVENTION = "manual_intervention"

@dataclass
class FailureEvent:
    """æ•…éšœäº‹ä»¶"""
    failure_type: FailureType
    severity: str
    description: str
    affected_services: List[str]
    timestamp: datetime
    metrics: Dict[str, Any]
    recovery_actions: List[RecoveryAction]

class FailureDetector:
    """æ•…éšœæ£€æµ‹å™¨"""
    
    def __init__(self):
        self.detection_rules = self.load_detection_rules()
        self.logger = logging.getLogger(__name__)
    
    def load_detection_rules(self) -> Dict[FailureType, Dict]:
        """åŠ è½½æ•…éšœæ£€æµ‹è§„åˆ™"""
        return {
            FailureType.SERVICE_DOWN: {
                "condition": "health_check_failures > 3",
                "window": "2m",
                "severity": "critical"
            },
            FailureType.HIGH_ERROR_RATE: {
                "condition": "error_rate > 10",
                "window": "5m",
                "severity": "high"
            },
            FailureType.SLOW_RESPONSE: {
                "condition": "response_time_p95 > 10",
                "window": "5m",
                "severity": "medium"
            },
            FailureType.MEMORY_LEAK: {
                "condition": "memory_growth_rate > 5",
                "window": "30m",
                "severity": "high"
            }
        }
    
    async def detect_failures(self, metrics: Dict[str, Any]) -> List[FailureEvent]:
        """æ£€æµ‹æ•…éšœ"""
        failures = []
        
        for failure_type, rule in self.detection_rules.items():
            if self.evaluate_condition(rule["condition"], metrics):
                failure = FailureEvent(
                    failure_type=failure_type,
                    severity=rule["severity"],
                    description=f"æ£€æµ‹åˆ°{failure_type.value}æ•…éšœ",
                    affected_services=self.identify_affected_services(failure_type, metrics),
                    timestamp=datetime.now(),
                    metrics=metrics,
                    recovery_actions=self.determine_recovery_actions(failure_type)
                )
                failures.append(failure)
        
        return failures
    
    def determine_recovery_actions(self, failure_type: FailureType) -> List[RecoveryAction]:
        """ç¡®å®šæ¢å¤åŠ¨ä½œ"""
        recovery_map = {
            FailureType.SERVICE_DOWN: [RecoveryAction.RESTART_SERVICE, RecoveryAction.FAILOVER],
            FailureType.HIGH_ERROR_RATE: [RecoveryAction.CIRCUIT_BREAKER, RecoveryAction.ROLLBACK],
            FailureType.SLOW_RESPONSE: [RecoveryAction.SCALE_UP, RecoveryAction.CIRCUIT_BREAKER],
            FailureType.MEMORY_LEAK: [RecoveryAction.RESTART_SERVICE],
            FailureType.DATABASE_CONNECTION: [RecoveryAction.FAILOVER, RecoveryAction.RESTART_SERVICE],
            FailureType.NETWORK_ISSUE: [RecoveryAction.FAILOVER, RecoveryAction.MANUAL_INTERVENTION]
        }
        return recovery_map.get(failure_type, [RecoveryAction.MANUAL_INTERVENTION])

class AutoRecoverySystem:
    """è‡ªåŠ¨æ¢å¤ç³»ç»Ÿ"""
    
    def __init__(self):
        self.failure_detector = FailureDetector()
        self.recovery_executor = RecoveryExecutor()
        self.logger = logging.getLogger(__name__)
    
    async def monitor_and_recover(self):
        """ç›‘æ§å’Œæ¢å¤ä¸»å¾ªç¯"""
        while True:
            try:
                # æ”¶é›†ç³»ç»ŸæŒ‡æ ‡
                metrics = await self.collect_system_metrics()
                
                # æ£€æµ‹æ•…éšœ
                failures = await self.failure_detector.detect_failures(metrics)
                
                # æ‰§è¡Œæ¢å¤åŠ¨ä½œ
                for failure in failures:
                    await self.handle_failure(failure)
                
                await asyncio.sleep(30)
                
            except Exception as e:
                self.logger.error(f"ç›‘æ§æ¢å¤ç³»ç»Ÿé”™è¯¯: {e}")
                await asyncio.sleep(60)
    
    async def handle_failure(self, failure: FailureEvent):
        """å¤„ç†æ•…éšœ"""
        self.logger.warning(f"æ£€æµ‹åˆ°æ•…éšœ: {failure}")
        
        for action in failure.recovery_actions:
            try:
                success = await self.recovery_executor.execute_action(action, failure)
                if success:
                    self.logger.info(f"æ¢å¤åŠ¨ä½œ {action} æ‰§è¡ŒæˆåŠŸ")
                    break
                else:
                    self.logger.warning(f"æ¢å¤åŠ¨ä½œ {action} æ‰§è¡Œå¤±è´¥")
            except Exception as e:
                self.logger.error(f"æ‰§è¡Œæ¢å¤åŠ¨ä½œ {action} æ—¶å‡ºé”™: {e}")
        
        # å‘é€å‘Šè­¦é€šçŸ¥
        await self.send_failure_notification(failure)

class RecoveryExecutor:
    """æ¢å¤æ‰§è¡Œå™¨"""
    
    async def execute_action(self, action: RecoveryAction, failure: FailureEvent) -> bool:
        """æ‰§è¡Œæ¢å¤åŠ¨ä½œ"""
        if action == RecoveryAction.RESTART_SERVICE:
            return await self.restart_service(failure.affected_services)
        elif action == RecoveryAction.SCALE_UP:
            return await self.scale_up_service(failure.affected_services)
        elif action == RecoveryAction.FAILOVER:
            return await self.failover_service(failure.affected_services)
        elif action == RecoveryAction.CIRCUIT_BREAKER:
            return await self.enable_circuit_breaker(failure.affected_services)
        elif action == RecoveryAction.ROLLBACK:
            return await self.rollback_deployment(failure.affected_services)
        else:
            return False
    
    async def restart_service(self, services: List[str]) -> bool:
        """é‡å¯æœåŠ¡"""
        try:
            for service in services:
                # ä½¿ç”¨ Kubernetes API é‡å¯æœåŠ¡
                await self.kubernetes_restart_deployment(service)
            return True
        except Exception as e:
            logging.error(f"é‡å¯æœåŠ¡å¤±è´¥: {e}")
            return False
    
    async def kubernetes_restart_deployment(self, deployment_name: str):
        """Kubernetes é‡å¯éƒ¨ç½²"""
        # å®é™…çš„ Kubernetes API è°ƒç”¨
        pass
```

### 3.4 æ€§èƒ½ä¼˜åŒ–å®è·µ

#### 3.4.1 ç³»ç»Ÿæ€§èƒ½è°ƒä¼˜

```python
# åŸºäºå®é™…é¡¹ç›®çš„æ€§èƒ½ä¼˜åŒ–å®ç°
import asyncio
import cProfile
import pstats
import io
import logging
from typing import Dict, List, Any, Optional
from dataclasses import dataclass
from datetime import datetime
import psutil
import gc
from prometheus_client import Counter, Gauge, Histogram

# Prometheus æŒ‡æ ‡å®šä¹‰
REQUEST_COUNT = Counter('requests_total', 'Total requests', ['method', 'endpoint'])
ACTIVE_CONVERSATIONS = Gauge('active_conversations', 'Number of active conversations')
RESPONSE_TIME = Histogram('response_time_seconds', 'Response time in seconds')

def monitor_performance(operation_name: str):
    """æ€§èƒ½ç›‘æ§è£…é¥°å™¨"""
    def decorator(func):
        async def wrapper(*args, **kwargs):
            start_time = datetime.now()
            try:
                result = await func(*args, **kwargs)
                return result
            finally:
                duration = (datetime.now() - start_time).total_seconds()
                RESPONSE_TIME.observe(duration)
                logging.info(f"{operation_name} æ‰§è¡Œæ—¶é—´: {duration:.2f}ç§’")
        return wrapper
    return decorator

@dataclass
class PerformanceProfile:
    """æ€§èƒ½åˆ†æç»“æœ"""
    function_stats: Dict[str, Any]
    memory_usage: Dict[str, float]
    cpu_usage: float
    io_stats: Dict[str, int]
    bottlenecks: List[str]
    recommendations: List[str]

class PerformanceOptimizer:
    """æ€§èƒ½ä¼˜åŒ–å™¨"""
    
    def __init__(self):
        self.profiler = cProfile.Profile()
        self.optimization_history = []
        self.logger = logging.getLogger(__name__)
    
    async def profile_system_performance(self) -> PerformanceProfile:
        """åˆ†æç³»ç»Ÿæ€§èƒ½"""
        # å¼€å§‹æ€§èƒ½åˆ†æ
        self.profiler.enable()
        
        # è¿è¡Œä¸€æ®µæ—¶é—´æ”¶é›†æ•°æ®
        await asyncio.sleep(60)
        
        # åœæ­¢åˆ†æ
        self.profiler.disable()
        
        # åˆ†æç»“æœ
        s = io.StringIO()
        ps = pstats.Stats(self.profiler, stream=s)
        ps.sort_stats('cumulative')
        ps.print_stats()
        
        # æ”¶é›†ç³»ç»ŸæŒ‡æ ‡
        memory_info = psutil.virtual_memory()
        cpu_usage = psutil.cpu_percent()
        io_counters = psutil.disk_io_counters()
        
        # è¯†åˆ«ç“¶é¢ˆ
        bottlenecks = self.identify_bottlenecks(ps)
        
        # ç”Ÿæˆä¼˜åŒ–å»ºè®®
        recommendations = self.generate_optimization_recommendations(bottlenecks)
        
        return PerformanceProfile(
            function_stats=self.extract_function_stats(ps),
            memory_usage={
                "total": memory_info.total,
                "used": memory_info.used,
                "percent": memory_info.percent
            },
            cpu_usage=cpu_usage,
            io_stats={
                "read_bytes": io_counters.read_bytes,
                "write_bytes": io_counters.write_bytes
            },
            bottlenecks=bottlenecks,
            recommendations=recommendations
        )
    
    def identify_bottlenecks(self, stats: pstats.Stats) -> List[str]:
        """è¯†åˆ«æ€§èƒ½ç“¶é¢ˆ"""
        bottlenecks = []
        
        # åˆ†ææœ€è€—æ—¶çš„å‡½æ•°
        stats.sort_stats('cumulative')
        top_functions = stats.get_stats_profile().func_profiles
        
        for func, (cc, nc, tt, ct, callers) in list(top_functions.items())[:10]:
            if ct > 1.0:  # ç´¯è®¡æ—¶é—´è¶…è¿‡1ç§’
                bottlenecks.append(f"å‡½æ•° {func} è€—æ—¶è¿‡é•¿: {ct:.2f}ç§’")
        
        return bottlenecks
    
    def generate_optimization_recommendations(self, bottlenecks: List[str]) -> List[str]:
        """ç”Ÿæˆä¼˜åŒ–å»ºè®®"""
        recommendations = []
        
        for bottleneck in bottlenecks:
            if "æ•°æ®åº“" in bottleneck:
                recommendations.append("ä¼˜åŒ–æ•°æ®åº“æŸ¥è¯¢ï¼Œæ·»åŠ ç´¢å¼•æˆ–ä½¿ç”¨ç¼“å­˜")
            elif "ç½‘ç»œ" in bottleneck:
                recommendations.append("ä¼˜åŒ–ç½‘ç»œè¯·æ±‚ï¼Œä½¿ç”¨è¿æ¥æ± æˆ–å¼‚æ­¥å¤„ç†")
            elif "å†…å­˜" in bottleneck:
                recommendations.append("ä¼˜åŒ–å†…å­˜ä½¿ç”¨ï¼Œæ£€æŸ¥å†…å­˜æ³„æ¼")
            elif "CPU" in bottleneck:
                recommendations.append("ä¼˜åŒ–ç®—æ³•å¤æ‚åº¦ï¼Œä½¿ç”¨å¹¶è¡Œå¤„ç†")
            else:
                recommendations.append("è¿›ä¸€æ­¥åˆ†æå…·ä½“ç“¶é¢ˆåŸå› ")
        
        return recommendations
    
    async def apply_optimizations(self, profile: PerformanceProfile):
        """åº”ç”¨æ€§èƒ½ä¼˜åŒ–"""
        for recommendation in profile.recommendations:
            if "ç¼“å­˜" in recommendation:
                await self.optimize_caching()
            elif "æ•°æ®åº“" in recommendation:
                await self.optimize_database()
            elif "å†…å­˜" in recommendation:
                await self.optimize_memory()
            elif "å¹¶è¡Œ" in recommendation:
                await self.optimize_concurrency()
    
    async def optimize_caching(self):
        """ä¼˜åŒ–ç¼“å­˜ç­–ç•¥"""
        # å®ç°ç¼“å­˜ä¼˜åŒ–é€»è¾‘
        self.logger.info("åº”ç”¨ç¼“å­˜ä¼˜åŒ–ç­–ç•¥")
    
    async def optimize_database(self):
        """ä¼˜åŒ–æ•°æ®åº“æ€§èƒ½"""
        # å®ç°æ•°æ®åº“ä¼˜åŒ–é€»è¾‘
        self.logger.info("åº”ç”¨æ•°æ®åº“ä¼˜åŒ–ç­–ç•¥")
    
    async def optimize_memory(self):
        """ä¼˜åŒ–å†…å­˜ä½¿ç”¨"""
        # å¼ºåˆ¶åƒåœ¾å›æ”¶
        gc.collect()
        self.logger.info("æ‰§è¡Œå†…å­˜ä¼˜åŒ–")
    
    async def optimize_concurrency(self):
        """ä¼˜åŒ–å¹¶å‘å¤„ç†"""
        # å®ç°å¹¶å‘ä¼˜åŒ–é€»è¾‘
        self.logger.info("åº”ç”¨å¹¶å‘ä¼˜åŒ–ç­–ç•¥")
    
    def extract_function_stats(self, stats: pstats.Stats) -> Dict[str, Any]:
        """æå–å‡½æ•°ç»Ÿè®¡ä¿¡æ¯"""
        function_stats = {}
        stats_profile = stats.get_stats_profile()
        
        for func, (cc, nc, tt, ct, callers) in stats_profile.func_profiles.items():
            function_stats[str(func)] = {
                "call_count": cc,
                "total_time": tt,
                "cumulative_time": ct,
                "per_call_time": tt / cc if cc > 0 else 0
            }
        
        return function_stats

# æ€§èƒ½ç›‘æ§å’Œä¼˜åŒ–ä¸»å¾ªç¯
async def performance_optimization_loop():
    """æ€§èƒ½ä¼˜åŒ–ä¸»å¾ªç¯"""
    optimizer = PerformanceOptimizer()
    
    while True:
        try:
            # åˆ†ææ€§èƒ½
            profile = await optimizer.profile_system_performance()
            
            # è®°å½•åˆ†æç»“æœ
            optimizer.logger.info(f"æ€§èƒ½åˆ†æå®Œæˆ: {profile}")
            
            # åº”ç”¨ä¼˜åŒ–
            await optimizer.apply_optimizations(profile)
            
            # ç­‰å¾…ä¸‹ä¸€æ¬¡åˆ†æ
            await asyncio.sleep(3600)  # æ¯å°æ—¶åˆ†æä¸€æ¬¡
            
        except Exception as e:
            optimizer.logger.error(f"æ€§èƒ½ä¼˜åŒ–é”™è¯¯: {e}")
            await asyncio.sleep(1800)  # é”™è¯¯æ—¶ç­‰å¾…30åˆ†é’Ÿ

# ä½¿ç”¨ç¤ºä¾‹
async def run_performance_optimization():
    """è¿è¡Œæ€§èƒ½ä¼˜åŒ–ç¤ºä¾‹"""
    optimizer = PerformanceOptimizer()
    
    # å¯åŠ¨æ€§èƒ½ä¼˜åŒ–å¾ªç¯
    await performance_optimization_loop()

# å¸¦ç›‘æ§çš„å¯¹è¯ç®¡ç†å™¨ç¤ºä¾‹
class MonitoredDialogManager:
    """å¸¦ç›‘æ§çš„å¯¹è¯ç®¡ç†å™¨"""
    
    def __init__(self):
        self.logger = logging.getLogger(__name__)
    
    @monitor_performance("dialog_manager")
    async def process_message(self, user_id: str, message: str):
        """å¤„ç†ç”¨æˆ·æ¶ˆæ¯"""
        ACTIVE_CONVERSATIONS.inc()
        try:
            # æ¨¡æ‹Ÿæ¶ˆæ¯å¤„ç†é€»è¾‘
            result = await self._handle_message(user_id, message)
            REQUEST_COUNT.labels(method='POST', endpoint='/chat').inc()
            return result
        finally:
            ACTIVE_CONVERSATIONS.dec()
    
    async def _handle_message(self, user_id: str, message: str) -> str:
        """å¤„ç†æ¶ˆæ¯çš„å…·ä½“é€»è¾‘"""
        # è¿™é‡Œå®ç°å…·ä½“çš„æ¶ˆæ¯å¤„ç†é€»è¾‘
        await asyncio.sleep(0.1)  # æ¨¡æ‹Ÿå¤„ç†æ—¶é—´
        return f"å¤„ç†ç”¨æˆ· {user_id} çš„æ¶ˆæ¯: {message}"
```

### 3.2 æ—¥å¿—ç®¡ç†

#### 3.2.1 ç»“æ„åŒ–æ—¥å¿—é…ç½®

```python
import logging
import json
from datetime import datetime
from typing import Dict, Any

class StructuredLogger:
    """ç»“æ„åŒ–æ—¥å¿—è®°å½•å™¨"""
    
    def __init__(self, name: str):
        self.logger = logging.getLogger(name)
        self.logger.setLevel(logging.INFO)
        
        # é…ç½®å¤„ç†å™¨
        handler = logging.StreamHandler()
        formatter = logging.Formatter('%(message)s')
        handler.setFormatter(formatter)
        self.logger.addHandler(handler)
    
    def log_event(self, event_type: str, data: Dict[str, Any], level: str = "INFO"):
        """è®°å½•ç»“æ„åŒ–äº‹ä»¶"""
        log_entry = {
            "timestamp": datetime.utcnow().isoformat(),
            "event_type": event_type,
            "level": level,
            "data": data
        }
        
        if level == "ERROR":
            self.logger.error(json.dumps(log_entry))
        elif level == "WARNING":
            self.logger.warning(json.dumps(log_entry))
        else:
            self.logger.info(json.dumps(log_entry))

# ä½¿ç”¨ç¤ºä¾‹
logger = StructuredLogger("customer_service")

# è®°å½•å¯¹è¯äº‹ä»¶
logger.log_event("conversation_started", {
    "user_id": "user_123",
    "session_id": "session_456",
    "channel": "web"
})

# è®°å½•é”™è¯¯äº‹ä»¶
logger.log_event("agent_error", {
    "agent_type": "knowledge_retriever",
    "error_message": "Connection timeout",
    "user_id": "user_123"
}, level="ERROR")
```

### 3.3 æ€§èƒ½ä¼˜åŒ–

#### 3.3.1 ç¼“å­˜ç­–ç•¥

```python
import redis
import pickle
from typing import Optional, Any
import hashlib

class CacheManager:
    """ç¼“å­˜ç®¡ç†å™¨"""
    
    def __init__(self, redis_url: str):
        self.redis_client = redis.from_url(redis_url)
        self.default_ttl = 3600  # 1å°æ—¶
    
    def _generate_key(self, prefix: str, *args) -> str:
        """ç”Ÿæˆç¼“å­˜é”®"""
        key_data = f"{prefix}:{':'.join(map(str, args))}"
        return hashlib.md5(key_data.encode()).hexdigest()
    
    async def get(self, prefix: str, *args) -> Optional[Any]:
        """è·å–ç¼“å­˜"""
        key = self._generate_key(prefix, *args)
        data = self.redis_client.get(key)
        if data:
            return pickle.loads(data)
        return None
    
    async def set(self, prefix: str, value: Any, ttl: Optional[int] = None, *args):
        """è®¾ç½®ç¼“å­˜"""
        key = self._generate_key(prefix, *args)
        ttl = ttl or self.default_ttl
        self.redis_client.setex(key, ttl, pickle.dumps(value))
    
    def cache_result(self, prefix: str, ttl: Optional[int] = None):
        """ç¼“å­˜è£…é¥°å™¨"""
        def decorator(func):
            @functools.wraps(func)
            async def wrapper(*args, **kwargs):
                # ç”Ÿæˆç¼“å­˜é”®
                cache_key = self._generate_key(prefix, *args, **kwargs)
                
                # å°è¯•ä»ç¼“å­˜è·å–
                cached_result = await self.get(prefix, *args, **kwargs)
                if cached_result is not None:
                    return cached_result
                
                # æ‰§è¡Œå‡½æ•°å¹¶ç¼“å­˜ç»“æœ
                result = await func(*args, **kwargs)
                await self.set(prefix, result, ttl, *args, **kwargs)
                return result
            return wrapper
        return decorator

# åº”ç”¨ç¼“å­˜
cache_manager = CacheManager("redis://localhost:6379")

class CachedKnowledgeRetriever(KnowledgeRetrieverAgent):
    """å¸¦ç¼“å­˜çš„çŸ¥è¯†æ£€ç´¢å™¨"""
    
    @cache_manager.cache_result("knowledge_search", ttl=1800)
    async def retrieve_knowledge(self, query: str, top_k: int = 5):
        return await super().retrieve_knowledge(query, top_k)
```

## 4. å®¹å™¨åŒ–éƒ¨ç½²ä¸äº‘åŸç”Ÿè¿ç»´

### 4.1 å®Œæ•´å®¹å™¨åŒ–éƒ¨ç½²æ–¹æ¡ˆ

#### 4.1.1 å¤šé˜¶æ®µDockerfileä¼˜åŒ–

```dockerfile
# åŸºäºå®é™…é¡¹ç›®çš„å¤šé˜¶æ®µæ„å»ºDockerfile
FROM python:3.11-slim as builder

# è®¾ç½®å·¥ä½œç›®å½•
WORKDIR /app

# å®‰è£…ç³»ç»Ÿä¾èµ–
RUN apt-get update && apt-get install -y \
    gcc \
    g++ \
    && rm -rf /var/lib/apt/lists/*

# å¤åˆ¶ä¾èµ–æ–‡ä»¶
COPY requirements.txt .

# å®‰è£…Pythonä¾èµ–
RUN pip install --no-cache-dir --user -r requirements.txt

# ç”Ÿäº§é˜¶æ®µ
FROM python:3.11-slim as production

# åˆ›å»ºérootç”¨æˆ·
RUN groupadd -r appuser && useradd -r -g appuser appuser

# è®¾ç½®å·¥ä½œç›®å½•
WORKDIR /app

# ä»builderé˜¶æ®µå¤åˆ¶ä¾èµ–
COPY --from=builder /root/.local /home/appuser/.local

# å¤åˆ¶åº”ç”¨ä»£ç 
COPY --chown=appuser:appuser . .

# è®¾ç½®ç¯å¢ƒå˜é‡
ENV PATH=/home/appuser/.local/bin:$PATH
ENV PYTHONPATH=/app
ENV PYTHONUNBUFFERED=1

# åˆ‡æ¢åˆ°érootç”¨æˆ·
USER appuser

# å¥åº·æ£€æŸ¥
HEALTHCHECK --interval=30s --timeout=30s --start-period=5s --retries=3 \
    CMD python -c "import requests; requests.get('http://localhost:8000/health')"

# æš´éœ²ç«¯å£
EXPOSE 8000

# å¯åŠ¨å‘½ä»¤
CMD ["python", "main.py"]
```

#### 4.1.2 ç”Ÿäº§çº§Docker Composeé…ç½®

```yaml
# docker-compose.prod.yml - ç”Ÿäº§ç¯å¢ƒé…ç½®
version: '3.8'

services:
  # ä¸»åº”ç”¨æœåŠ¡
  customer-service:
    build:
      context: .
      dockerfile: Dockerfile
      target: production
    image: customer-service:latest
    container_name: customer-service-app
    restart: unless-stopped
    environment:
      - ENVIRONMENT=production
      - REDIS_URL=redis://redis:6379
      - DATABASE_URL=postgresql://user:password@postgres:5432/customer_service
      - LANGSMITH_API_KEY=${LANGSMITH_API_KEY}
      - OPENAI_API_KEY=${OPENAI_API_KEY}
    ports:
      - "8000:8000"
    depends_on:
      - redis
      - postgres
      - prometheus
    networks:
      - app-network
    volumes:
      - ./logs:/app/logs
      - ./config:/app/config:ro
    deploy:
      resources:
        limits:
          cpus: '2.0'
          memory: 4G
        reservations:
          cpus: '1.0'
          memory: 2G

  # Redisç¼“å­˜æœåŠ¡
  redis:
    image: redis:7-alpine
    container_name: customer-service-redis
    restart: unless-stopped
    command: redis-server --appendonly yes --requirepass ${REDIS_PASSWORD}
    ports:
      - "6379:6379"
    volumes:
      - redis-data:/data
    networks:
      - app-network
    deploy:
      resources:
        limits:
          memory: 1G

  # PostgreSQLæ•°æ®åº“
  postgres:
    image: postgres:15-alpine
    container_name: customer-service-db
    restart: unless-stopped
    environment:
      - POSTGRES_DB=customer_service
      - POSTGRES_USER=user
      - POSTGRES_PASSWORD=${POSTGRES_PASSWORD}
    ports:
      - "5432:5432"
    volumes:
      - postgres-data:/var/lib/postgresql/data
      - ./init.sql:/docker-entrypoint-initdb.d/init.sql:ro
    networks:
      - app-network
    deploy:
      resources:
        limits:
          memory: 2G

  # Nginxè´Ÿè½½å‡è¡¡å™¨
  nginx:
    image: nginx:alpine
    container_name: customer-service-nginx
    restart: unless-stopped
    ports:
      - "80:80"
      - "443:443"
    volumes:
      - ./nginx/nginx.conf:/etc/nginx/nginx.conf:ro
      - ./nginx/ssl:/etc/nginx/ssl:ro
      - ./logs/nginx:/var/log/nginx
    depends_on:
      - customer-service
    networks:
      - app-network

  # Prometheusç›‘æ§
  prometheus:
    image: prom/prometheus:latest
    container_name: customer-service-prometheus
    restart: unless-stopped
    ports:
      - "9090:9090"
    volumes:
      - ./monitoring/prometheus.yml:/etc/prometheus/prometheus.yml:ro
      - prometheus-data:/prometheus
    command:
      - '--config.file=/etc/prometheus/prometheus.yml'
      - '--storage.tsdb.path=/prometheus'
      - '--web.console.libraries=/etc/prometheus/console_libraries'
      - '--web.console.templates=/etc/prometheus/consoles'
      - '--storage.tsdb.retention.time=200h'
      - '--web.enable-lifecycle'
    networks:
      - app-network

  # Grafanaä»ªè¡¨æ¿
  grafana:
    image: grafana/grafana:latest
    container_name: customer-service-grafana
    restart: unless-stopped
    ports:
      - "3000:3000"
    environment:
      - GF_SECURITY_ADMIN_PASSWORD=${GRAFANA_PASSWORD}
    volumes:
      - grafana-data:/var/lib/grafana
      - ./monitoring/grafana/dashboards:/etc/grafana/provisioning/dashboards:ro
      - ./monitoring/grafana/datasources:/etc/grafana/provisioning/datasources:ro
    networks:
      - app-network

volumes:
  redis-data:
  postgres-data:
  prometheus-data:
  grafana-data:

networks:
  app-network:
    driver: bridge
```

### 4.2 Kubernetesäº‘åŸç”Ÿéƒ¨ç½²

#### 4.2.1 Kuberneteséƒ¨ç½²æ¸…å•

```yaml
# k8s/namespace.yaml
apiVersion: v1
kind: Namespace
metadata:
  name: customer-service
  labels:
    name: customer-service

---
# k8s/configmap.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: customer-service-config
  namespace: customer-service
data:
  ENVIRONMENT: "production"
  REDIS_URL: "redis://redis-service:6379"
  DATABASE_URL: "postgresql://user:password@postgres-service:5432/customer_service"

---
# k8s/secret.yaml
apiVersion: v1
kind: Secret
metadata:
  name: customer-service-secrets
  namespace: customer-service
type: Opaque
data:
  LANGSMITH_API_KEY: <base64-encoded-key>
  OPENAI_API_KEY: <base64-encoded-key>
  REDIS_PASSWORD: <base64-encoded-password>
  POSTGRES_PASSWORD: <base64-encoded-password>

---
# k8s/deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: customer-service-app
  namespace: customer-service
  labels:
    app: customer-service
spec:
  replicas: 3
  selector:
    matchLabels:
      app: customer-service
  template:
    metadata:
      labels:
        app: customer-service
    spec:
      containers:
      - name: customer-service
        image: customer-service:latest
        ports:
        - containerPort: 8000
        envFrom:
        - configMapRef:
            name: customer-service-config
        - secretRef:
            name: customer-service-secrets
        resources:
          requests:
            memory: "1Gi"
            cpu: "500m"
          limits:
            memory: "2Gi"
            cpu: "1000m"
        livenessProbe:
          httpGet:
            path: /health
            port: 8000
          initialDelaySeconds: 30
          periodSeconds: 10
        readinessProbe:
          httpGet:
            path: /ready
            port: 8000
          initialDelaySeconds: 5
          periodSeconds: 5
        volumeMounts:
        - name: logs
          mountPath: /app/logs
      volumes:
      - name: logs
        persistentVolumeClaim:
          claimName: customer-service-logs-pvc

---
# k8s/service.yaml
apiVersion: v1
kind: Service
metadata:
  name: customer-service-svc
  namespace: customer-service
spec:
  selector:
    app: customer-service
  ports:
  - protocol: TCP
    port: 80
    targetPort: 8000
  type: ClusterIP

---
# k8s/hpa.yaml
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: customer-service-hpa
  namespace: customer-service
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: customer-service-app
  minReplicas: 3
  maxReplicas: 10
  metrics:
  - type: Resource
    resource:
      name: cpu
      target:
        type: Utilization
        averageUtilization: 70
  - type: Resource
    resource:
      name: memory
      target:
        type: Utilization
        averageUtilization: 80
```

#### 4.2.2 Ingresså’ŒSSLé…ç½®

```yaml
# k8s/ingress.yaml
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: customer-service-ingress
  namespace: customer-service
  annotations:
    kubernetes.io/ingress.class: nginx
    cert-manager.io/cluster-issuer: letsencrypt-prod
    nginx.ingress.kubernetes.io/rate-limit: "100"
    nginx.ingress.kubernetes.io/rate-limit-window: "1m"
spec:
  tls:
  - hosts:
    - api.customer-service.com
    secretName: customer-service-tls
  rules:
  - host: api.customer-service.com
    http:
      paths:
      - path: /
        pathType: Prefix
        backend:
          service:
            name: customer-service-svc
            port:
              number: 80
```

### 4.3 äº‘åŸç”Ÿè¿ç»´è‡ªåŠ¨åŒ–

#### 4.3.1 GitOpséƒ¨ç½²æµæ°´çº¿

```yaml
# .github/workflows/deploy.yml
name: Deploy to Production

on:
  push:
    branches: [main]
  pull_request:
    branches: [main]

jobs:
  test:
    runs-on: ubuntu-latest
    steps:
    - uses: actions/checkout@v3
    
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.11'
    
    - name: Install dependencies
      run: |
        pip install -r requirements.txt
        pip install pytest pytest-cov
    
    - name: Run tests
      run: |
        pytest tests/ --cov=src/ --cov-report=xml
    
    - name: Upload coverage
      uses: codecov/codecov-action@v3

  build:
    needs: test
    runs-on: ubuntu-latest
    if: github.ref == 'refs/heads/main'
    steps:
    - uses: actions/checkout@v3
    
    - name: Set up Docker Buildx
      uses: docker/setup-buildx-action@v2
    
    - name: Login to Container Registry
      uses: docker/login-action@v2
      with:
        registry: ghcr.io
        username: ${{ github.actor }}
        password: ${{ secrets.GITHUB_TOKEN }}
    
    - name: Build and push
      uses: docker/build-push-action@v4
      with:
        context: .
        push: true
        tags: |
          ghcr.io/${{ github.repository }}:latest
          ghcr.io/${{ github.repository }}:${{ github.sha }}
        cache-from: type=gha
        cache-to: type=gha,mode=max

  deploy:
    needs: build
    runs-on: ubuntu-latest
    if: github.ref == 'refs/heads/main'
    steps:
    - uses: actions/checkout@v3
    
    - name: Setup kubectl
      uses: azure/setup-kubectl@v3
      with:
        version: 'v1.28.0'
    
    - name: Deploy to Kubernetes
      run: |
        echo "${{ secrets.KUBECONFIG }}" | base64 -d > kubeconfig
        export KUBECONFIG=kubeconfig
        
        # æ›´æ–°é•œåƒæ ‡ç­¾
        kubectl set image deployment/customer-service-app \
          customer-service=ghcr.io/${{ github.repository }}:${{ github.sha }} \
          -n customer-service
        
        # ç­‰å¾…éƒ¨ç½²å®Œæˆ
        kubectl rollout status deployment/customer-service-app -n customer-service
```

#### 4.3.2 è‡ªåŠ¨åŒ–è¿ç»´è„šæœ¬

```python
# scripts/auto_ops.py - è‡ªåŠ¨åŒ–è¿ç»´è„šæœ¬
import asyncio
import logging
from typing import Dict, List
from kubernetes import client, config
from prometheus_client.parser import text_string_to_metric_families
import requests

class CloudNativeOpsManager:
    """äº‘åŸç”Ÿè¿ç»´ç®¡ç†å™¨"""
    
    def __init__(self):
        # åŠ è½½Kubernetesé…ç½®
        try:
            config.load_incluster_config()  # é›†ç¾¤å†…é…ç½®
        except:
            config.load_kube_config()  # æœ¬åœ°é…ç½®
        
        self.k8s_apps_v1 = client.AppsV1Api()
        self.k8s_core_v1 = client.CoreV1Api()
        self.k8s_autoscaling_v2 = client.AutoscalingV2Api()
        
        self.logger = logging.getLogger(__name__)
        
        # è¿ç»´é…ç½®
        self.namespace = "customer-service"
        self.deployment_name = "customer-service-app"
        self.prometheus_url = "http://prometheus:9090"
    
    async def health_check(self) -> Dict[str, bool]:
        """ç³»ç»Ÿå¥åº·æ£€æŸ¥"""
        health_status = {}
        
        try:
            # æ£€æŸ¥PodçŠ¶æ€
            pods = self.k8s_core_v1.list_namespaced_pod(
                namespace=self.namespace,
                label_selector=f"app=customer-service"
            )
            
            healthy_pods = sum(1 for pod in pods.items 
                             if pod.status.phase == "Running")
            total_pods = len(pods.items)
            
            health_status["pods"] = healthy_pods == total_pods and total_pods > 0
            
            # æ£€æŸ¥æœåŠ¡çŠ¶æ€
            services = self.k8s_core_v1.list_namespaced_service(
                namespace=self.namespace
            )
            health_status["services"] = len(services.items) > 0
            
            # æ£€æŸ¥åº”ç”¨å“åº”
            try:
                response = requests.get(
                    f"http://customer-service-svc.{self.namespace}/health",
                    timeout=10
                )
                health_status["application"] = response.status_code == 200
            except:
                health_status["application"] = False
            
            self.logger.info(f"å¥åº·æ£€æŸ¥å®Œæˆ: {health_status}")
            return health_status
            
        except Exception as e:
            self.logger.error(f"å¥åº·æ£€æŸ¥å¤±è´¥: {e}")
            return {"error": True}
    
    async def auto_scale_decision(self) -> Dict[str, any]:
        """è‡ªåŠ¨æ‰©ç¼©å®¹å†³ç­–"""
        try:
            # è·å–å½“å‰HPAçŠ¶æ€
            hpa = self.k8s_autoscaling_v2.read_namespaced_horizontal_pod_autoscaler(
                name="customer-service-hpa",
                namespace=self.namespace
            )
            
            current_replicas = hpa.status.current_replicas
            desired_replicas = hpa.status.desired_replicas
            
            # è·å–æ€§èƒ½æŒ‡æ ‡
            metrics = await self.get_performance_metrics()
            
            # æ‰©ç¼©å®¹å†³ç­–é€»è¾‘
            decision = {
                "current_replicas": current_replicas,
                "desired_replicas": desired_replicas,
                "cpu_usage": metrics.get("cpu_usage", 0),
                "memory_usage": metrics.get("memory_usage", 0),
                "request_rate": metrics.get("request_rate", 0),
                "action": "none"
            }
            
            # åŸºäºä¸šåŠ¡æŒ‡æ ‡çš„æ™ºèƒ½æ‰©ç¼©å®¹
            if metrics.get("request_rate", 0) > 1000 and current_replicas < 8:
                decision["action"] = "scale_up"
                decision["reason"] = "High request rate detected"
            elif metrics.get("cpu_usage", 0) > 80 and current_replicas < 10:
                decision["action"] = "scale_up"
                decision["reason"] = "High CPU usage"
            elif (metrics.get("request_rate", 0) < 100 and 
                  metrics.get("cpu_usage", 0) < 30 and 
                  current_replicas > 3):
                decision["action"] = "scale_down"
                decision["reason"] = "Low resource usage"
            
            return decision
            
        except Exception as e:
            self.logger.error(f"æ‰©ç¼©å®¹å†³ç­–å¤±è´¥: {e}")
            return {"error": str(e)}
    
    async def get_performance_metrics(self) -> Dict[str, float]:
        """è·å–æ€§èƒ½æŒ‡æ ‡"""
        try:
            metrics = {}
            
            # CPUä½¿ç”¨ç‡
            cpu_query = f'avg(rate(container_cpu_usage_seconds_total{{namespace="{self.namespace}"}}[5m])) * 100'
            cpu_result = await self.query_prometheus(cpu_query)
            metrics["cpu_usage"] = float(cpu_result[0]["value"][1]) if cpu_result else 0
            
            # å†…å­˜ä½¿ç”¨ç‡
            memory_query = f'avg(container_memory_usage_bytes{{namespace="{self.namespace}"}}) / avg(container_spec_memory_limit_bytes{{namespace="{self.namespace}"}}) * 100'
            memory_result = await self.query_prometheus(memory_query)
            metrics["memory_usage"] = float(memory_result[0]["value"][1]) if memory_result else 0
            
            # è¯·æ±‚é€Ÿç‡
            request_query = f'sum(rate(http_requests_total{{namespace="{self.namespace}"}}[5m]))'
            request_result = await self.query_prometheus(request_query)
            metrics["request_rate"] = float(request_result[0]["value"][1]) if request_result else 0
            
            return metrics
            
        except Exception as e:
            self.logger.error(f"è·å–æ€§èƒ½æŒ‡æ ‡å¤±è´¥: {e}")
            return {}
    
    async def query_prometheus(self, query: str) -> List[Dict]:
        """æŸ¥è¯¢PrometheusæŒ‡æ ‡"""
        try:
            response = requests.get(
                f"{self.prometheus_url}/api/v1/query",
                params={"query": query},
                timeout=10
            )
            data = response.json()
            return data.get("data", {}).get("result", [])
        except Exception as e:
            self.logger.error(f"PrometheusæŸ¥è¯¢å¤±è´¥: {e}")
            return []
    
    async def rolling_update(self, new_image: str):
        """æ»šåŠ¨æ›´æ–°éƒ¨ç½²"""
        try:
            # è·å–å½“å‰éƒ¨ç½²
            deployment = self.k8s_apps_v1.read_namespaced_deployment(
                name=self.deployment_name,
                namespace=self.namespace
            )
            
            # æ›´æ–°é•œåƒ
            deployment.spec.template.spec.containers[0].image = new_image
            
            # åº”ç”¨æ›´æ–°
            self.k8s_apps_v1.patch_namespaced_deployment(
                name=self.deployment_name,
                namespace=self.namespace,
                body=deployment
            )
            
            self.logger.info(f"å¼€å§‹æ»šåŠ¨æ›´æ–°åˆ°é•œåƒ: {new_image}")
            
            # ç­‰å¾…æ›´æ–°å®Œæˆ
            await self.wait_for_rollout()
            
        except Exception as e:
            self.logger.error(f"æ»šåŠ¨æ›´æ–°å¤±è´¥: {e}")
            raise
    
    async def wait_for_rollout(self, timeout: int = 600):
        """ç­‰å¾…æ»šåŠ¨æ›´æ–°å®Œæˆ"""
        import time
        start_time = time.time()
        
        while time.time() - start_time < timeout:
            try:
                deployment = self.k8s_apps_v1.read_namespaced_deployment(
                    name=self.deployment_name,
                    namespace=self.namespace
                )
                
                if (deployment.status.ready_replicas == deployment.spec.replicas and
                    deployment.status.updated_replicas == deployment.spec.replicas):
                    self.logger.info("æ»šåŠ¨æ›´æ–°å®Œæˆ")
                    return True
                
                await asyncio.sleep(10)
                
            except Exception as e:
                self.logger.error(f"æ£€æŸ¥æ»šåŠ¨æ›´æ–°çŠ¶æ€å¤±è´¥: {e}")
                await asyncio.sleep(10)
        
        raise TimeoutError("æ»šåŠ¨æ›´æ–°è¶…æ—¶")

# è¿ç»´ä¸»å¾ªç¯
async def ops_main_loop():
    """è¿ç»´ä¸»å¾ªç¯"""
    ops_manager = CloudNativeOpsManager()
    
    while True:
        try:
            # å¥åº·æ£€æŸ¥
            health = await ops_manager.health_check()
            
            if not all(health.values()):
                logging.warning(f"ç³»ç»Ÿå¥åº·æ£€æŸ¥å¼‚å¸¸: {health}")
            
            # è‡ªåŠ¨æ‰©ç¼©å®¹å†³ç­–
            scale_decision = await ops_manager.auto_scale_decision()
            
            if scale_decision.get("action") != "none":
                logging.info(f"æ‰©ç¼©å®¹å†³ç­–: {scale_decision}")
            
            # ç­‰å¾…ä¸‹ä¸€æ¬¡æ£€æŸ¥
            await asyncio.sleep(60)  # æ¯åˆ†é’Ÿæ£€æŸ¥ä¸€æ¬¡
            
        except Exception as e:
            logging.error(f"è¿ç»´å¾ªç¯é”™è¯¯: {e}")
            await asyncio.sleep(60)

if __name__ == "__main__":
    logging.basicConfig(level=logging.INFO)
    asyncio.run(ops_main_loop())
```

## 5. å®è·µç»ƒä¹ 

### ç»ƒä¹ 1ï¼šå®Œæ•´ç³»ç»Ÿéƒ¨ç½²

1. ä½¿ç”¨Docker Composeéƒ¨ç½²å®Œæ•´çš„å®¢æœç³»ç»Ÿ
2. é…ç½®Nginxè´Ÿè½½å‡è¡¡
3. è®¾ç½®SSLè¯ä¹¦
4. éªŒè¯ç³»ç»ŸåŠŸèƒ½

### ç»ƒä¹ 2ï¼šKubernetesäº‘åŸç”Ÿéƒ¨ç½²

1. éƒ¨ç½²åˆ°Kubernetesé›†ç¾¤
2. é…ç½®è‡ªåŠ¨æ‰©ç¼©å®¹
3. è®¾ç½®æ»šåŠ¨æ›´æ–°
4. éªŒè¯é«˜å¯ç”¨æ€§

### ç»ƒä¹ 3ï¼šç›‘æ§å‘Šè­¦é…ç½®

1. é…ç½®Prometheusç›‘æ§æŒ‡æ ‡
2. è®¾ç½®Grafanaä»ªè¡¨æ¿
3. é…ç½®å‘Šè­¦è§„åˆ™
4. æµ‹è¯•å‘Šè­¦æœºåˆ¶

### ç»ƒä¹ 4ï¼šæ€§èƒ½ä¼˜åŒ–å®æˆ˜

1. è¿›è¡Œå‹åŠ›æµ‹è¯•
2. è¯†åˆ«æ€§èƒ½ç“¶é¢ˆ
3. å®æ–½ä¼˜åŒ–æ–¹æ¡ˆ
4. éªŒè¯ä¼˜åŒ–æ•ˆæœ

## è¯¾ç¨‹æ€»ç»“

### å…³é”®æ”¶è·

1. **å®Œæ•´å¼€å‘æµç¨‹**ï¼šä»éœ€æ±‚åˆ†æåˆ°ç”Ÿäº§éƒ¨ç½²çš„å…¨æµç¨‹å®è·µ
2. **ä¼ä¸šçº§æ¶æ„**ï¼šå¯æ‰©å±•ã€é«˜å¯ç”¨çš„ç³»ç»Ÿæ¶æ„è®¾è®¡
3. **è¿ç»´æœ€ä½³å®è·µ**ï¼šç›‘æ§ã€æ—¥å¿—ã€æ€§èƒ½ä¼˜åŒ–çš„å®æˆ˜ç»éªŒ
4. **é—®é¢˜è§£å†³èƒ½åŠ›**ï¼šå…·å¤‡ç‹¬ç«‹è§£å†³å¤æ‚æŠ€æœ¯é—®é¢˜çš„èƒ½åŠ›

### åç»­å­¦ä¹ å»ºè®®

1. æ·±å…¥å­¦ä¹ äº‘åŸç”ŸæŠ€æœ¯ï¼ˆKubernetesã€Service Meshï¼‰
2. æ¢ç´¢æ›´å¤šAIæŠ€æœ¯ï¼ˆå¤šæ¨¡æ€ã€å¼ºåŒ–å­¦ä¹ ï¼‰
3. å…³æ³¨è¡Œä¸šæœ€æ–°å‘å±•ï¼ˆAGIã€å…·èº«æ™ºèƒ½ï¼‰
4. å‚ä¸å¼€æºé¡¹ç›®ï¼Œç§¯ç´¯å®æˆ˜ç»éªŒ

### è®¤è¯è€ƒæ ¸

- å®Œæˆç»¼åˆé¡¹ç›®å®æˆ˜
- é€šè¿‡æŠ€æœ¯ç­”è¾©
- è·å¾—ä¼ä¸šçº§å¤šæ™ºèƒ½ä½“ç³»ç»Ÿå¼€å‘è®¤è¯

---

**æ­å–œæ‚¨å®Œæˆäº†å¤šæ™ºèƒ½ä½“AIç³»ç»ŸåŸ¹è®­çš„å…¨éƒ¨è¯¾ç¨‹ï¼**
