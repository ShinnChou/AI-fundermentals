#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
LLMå·¥å‚ç±» - æ ¹æ®é…ç½®åˆ›å»ºä¸åŒçš„LLMå®ä¾‹
"""

import os
from typing import Optional, Union
from langchain_openai import ChatOpenAI
from langchain_community.llms import Ollama
from langchain_core.language_models.base import BaseLanguageModel

try:
    from .config import config, check_config
except ImportError:
    from config import config, check_config

class LLMFactory:
    """LLMå·¥å‚ç±»ï¼Œç”¨äºåˆ›å»ºä¸åŒç±»å‹çš„è¯­è¨€æ¨¡å‹å®ä¾‹"""
    
    @staticmethod
    def create_openai_llm(
        api_key: Optional[str] = None,
        base_url: Optional[str] = None,
        model: Optional[str] = None,
        temperature: Optional[float] = None,
        max_tokens: Optional[int] = None,
        timeout: Optional[int] = None
    ) -> ChatOpenAI:
        """
        åˆ›å»ºOpenAIå…¼å®¹çš„LLMå®ä¾‹
        
        Args:
            api_key: APIå¯†é’¥
            base_url: APIåŸºç¡€URL
            model: æ¨¡å‹åç§°
            temperature: æ¸©åº¦å‚æ•°
            max_tokens: æœ€å¤§tokenæ•°
            timeout: è¶…æ—¶æ—¶é—´
            
        Returns:
            ChatOpenAIå®ä¾‹
        """
        # ä½¿ç”¨ä¼ å…¥å‚æ•°æˆ–é…ç½®æ–‡ä»¶ä¸­çš„é»˜è®¤å€¼
        api_key = api_key or config.openai_api_key
        base_url = base_url or config.openai_base_url
        model = model or config.openai_model
        temperature = temperature if temperature is not None else config.temperature
        max_tokens = max_tokens or config.max_tokens
        timeout = timeout or config.timeout
        
        # éªŒè¯å¿…è¦å‚æ•°
        if not api_key:
            raise ValueError("API Key ä¸èƒ½ä¸ºç©ºï¼è¯·è®¾ç½® OPENAI_API_KEY ç¯å¢ƒå˜é‡æˆ–åœ¨é…ç½®æ–‡ä»¶ä¸­æŒ‡å®š")
        
        return ChatOpenAI(
            api_key=api_key,
            base_url=base_url,
            model=model,
            temperature=temperature,
            max_tokens=max_tokens,
            timeout=timeout
        )
    
    @staticmethod
    def create_local_llm(
        base_url: Optional[str] = None,
        model: Optional[str] = None,
        api_key: Optional[str] = None,
        temperature: Optional[float] = None,
        max_tokens: Optional[int] = None,
        timeout: Optional[int] = None
    ) -> ChatOpenAI:
        """
        åˆ›å»ºæœ¬åœ°éƒ¨ç½²çš„LLMå®ä¾‹ï¼ˆä½¿ç”¨OpenAIå…¼å®¹æ¥å£ï¼‰
        
        Args:
            base_url: æœ¬åœ°APIåœ°å€
            model: æ¨¡å‹åç§°
            api_key: APIå¯†é’¥ï¼ˆå¦‚æœéœ€è¦ï¼‰
            temperature: æ¸©åº¦å‚æ•°
            max_tokens: æœ€å¤§tokenæ•°
            timeout: è¶…æ—¶æ—¶é—´
            
        Returns:
            ChatOpenAIå®ä¾‹
        """
        # ä½¿ç”¨ä¼ å…¥å‚æ•°æˆ–é…ç½®æ–‡ä»¶ä¸­çš„é»˜è®¤å€¼
        base_url = base_url or config.local_base_url
        model = model or config.local_model
        api_key = api_key or config.local_api_key or "dummy-key"  # æœ¬åœ°æ¨¡å‹å¯èƒ½ä¸éœ€è¦çœŸå®çš„API key
        temperature = temperature if temperature is not None else config.temperature
        max_tokens = max_tokens or config.max_tokens
        timeout = timeout or config.timeout
        
        # éªŒè¯å¿…è¦å‚æ•°
        if not base_url:
            raise ValueError("æœ¬åœ°æ¨¡å‹URLä¸èƒ½ä¸ºç©ºï¼è¯·è®¾ç½® LOCAL_BASE_URL ç¯å¢ƒå˜é‡æˆ–åœ¨é…ç½®æ–‡ä»¶ä¸­æŒ‡å®š")
        
        return ChatOpenAI(
            api_key=api_key,
            base_url=base_url,
            model=model,
            temperature=temperature,
            max_tokens=max_tokens,
            timeout=timeout
        )
    
    @staticmethod
    def create_ollama_llm(
        model: Optional[str] = None,
        base_url: Optional[str] = None,
        temperature: Optional[float] = None
    ) -> Ollama:
        """
        åˆ›å»ºOllama LLMå®ä¾‹
        
        Args:
            model: æ¨¡å‹åç§°
            base_url: OllamaæœåŠ¡åœ°å€
            temperature: æ¸©åº¦å‚æ•°
            
        Returns:
            Ollamaå®ä¾‹
        """
        model = model or config.local_model
        base_url = base_url or "http://localhost:11434"  # Ollamaé»˜è®¤åœ°å€
        temperature = temperature if temperature is not None else config.temperature
        
        return Ollama(
            model=model,
            base_url=base_url,
            temperature=temperature
        )
    
    @staticmethod
    def create_deepseek_llm(
        api_key: Optional[str] = None,
        model: Optional[str] = None,
        temperature: Optional[float] = None,
        max_tokens: Optional[int] = None,
        timeout: Optional[int] = None
    ) -> ChatOpenAI:
        """
        åˆ›å»ºDeepSeek LLMå®ä¾‹
        
        Args:
            api_key: DeepSeek APIå¯†é’¥
            model: æ¨¡å‹åç§°
            temperature: æ¸©åº¦å‚æ•°
            max_tokens: æœ€å¤§tokenæ•°
            timeout: è¶…æ—¶æ—¶é—´
            
        Returns:
            ChatOpenAIå®ä¾‹
        """
        # ä½¿ç”¨ä¼ å…¥å‚æ•°æˆ–é…ç½®æ–‡ä»¶ä¸­çš„é»˜è®¤å€¼
        api_key = api_key or config.deepseek_api_key
        model = model or config.deepseek_model or "deepseek-chat"
        temperature = temperature if temperature is not None else config.temperature
        max_tokens = max_tokens or config.max_tokens
        timeout = timeout or config.timeout
        
        # éªŒè¯å¿…è¦å‚æ•°
        if not api_key:
            raise ValueError("DeepSeek API Key ä¸èƒ½ä¸ºç©ºï¼è¯·è®¾ç½® DEEPSEEK_API_KEY ç¯å¢ƒå˜é‡æˆ–åœ¨é…ç½®æ–‡ä»¶ä¸­æŒ‡å®š")
        
        return ChatOpenAI(
            api_key=api_key,
            base_url="https://api.deepseek.com",
            model=model,
            temperature=temperature,
            max_tokens=max_tokens,
            timeout=timeout
        )
    
    @staticmethod
    def create_llm(
        provider: str = "openai",
        **kwargs
    ) -> BaseLanguageModel:
        """
        æ ¹æ®æä¾›å•†åˆ›å»ºLLMå®ä¾‹
        
        Args:
            provider: æä¾›å•†ç±»å‹ ("openai", "deepseek", "local", "ollama")
            **kwargs: å…¶ä»–å‚æ•°
            
        Returns:
            LLMå®ä¾‹
        """
        provider = provider.lower()
        
        if provider == "openai":
            return LLMFactory.create_openai_llm(**kwargs)
        elif provider == "deepseek":
            return LLMFactory.create_deepseek_llm(**kwargs)
        elif provider == "local":
            return LLMFactory.create_local_llm(**kwargs)
        elif provider == "ollama":
            return LLMFactory.create_ollama_llm(**kwargs)
        else:
            raise ValueError(f"ä¸æ”¯æŒçš„æä¾›å•†: {provider}ã€‚æ”¯æŒçš„æä¾›å•†: openai, deepseek, local, ollama")
    
    @staticmethod
    def auto_create_llm() -> BaseLanguageModel:
        """
        è‡ªåŠ¨é€‰æ‹©å¯ç”¨çš„LLMå®ä¾‹
        
        ä¼˜å…ˆçº§ï¼šOpenAI > DeepSeek > æœ¬åœ°æ¨¡å‹ > Ollama
        
        Returns:
            LLMå®ä¾‹
        """
        # å°è¯•OpenAI
        try:
            if config.validate_config("openai"):
                print("ğŸ¤– ä½¿ç”¨ OpenAI æ¨¡å‹")
                return LLMFactory.create_openai_llm()
        except Exception as e:
            print(f"âš ï¸ OpenAI æ¨¡å‹åˆ›å»ºå¤±è´¥: {e}")
        
        # å°è¯•DeepSeek
        try:
            if config.validate_config("deepseek"):
                print("ğŸ¤– ä½¿ç”¨ DeepSeek æ¨¡å‹")
                return LLMFactory.create_deepseek_llm()
        except Exception as e:
            print(f"âš ï¸ DeepSeek æ¨¡å‹åˆ›å»ºå¤±è´¥: {e}")
        
        # å°è¯•æœ¬åœ°æ¨¡å‹
        try:
            if config.validate_config("local"):
                print("ğŸ¤– ä½¿ç”¨æœ¬åœ°æ¨¡å‹")
                return LLMFactory.create_local_llm()
        except Exception as e:
            print(f"âš ï¸ æœ¬åœ°æ¨¡å‹åˆ›å»ºå¤±è´¥: {e}")
        
        # å°è¯•Ollama
        try:
            print("ğŸ¤– å°è¯•ä½¿ç”¨ Ollama æ¨¡å‹")
            return LLMFactory.create_ollama_llm()
        except Exception as e:
            print(f"âš ï¸ Ollama æ¨¡å‹åˆ›å»ºå¤±è´¥: {e}")
        
        raise RuntimeError(
            "æ— æ³•åˆ›å»ºä»»ä½•LLMå®ä¾‹ï¼è¯·æ£€æŸ¥é…ç½®ï¼š\n"
            "1. è®¾ç½® OPENAI_API_KEY ç¯å¢ƒå˜é‡\n"
            "2. æˆ–è®¾ç½® DEEPSEEK_API_KEY ç¯å¢ƒå˜é‡\n"
            "3. æˆ–é…ç½®æœ¬åœ°æ¨¡å‹ LOCAL_BASE_URL\n"
            "4. æˆ–å¯åŠ¨ Ollama æœåŠ¡"
        )

def get_openai_llm(**kwargs):
    """
    è·å–OpenAI LLMå®ä¾‹
    
    Args:
        **kwargs: é¢å¤–å‚æ•°
    
    Returns:
        ChatOpenAIå®ä¾‹
    """
    return LLMFactory.create_openai_llm(**kwargs)

def get_deepseek_llm(**kwargs):
    """
    è·å–DeepSeek LLMå®ä¾‹
    
    Args:
        **kwargs: é¢å¤–å‚æ•°
    
    Returns:
        ChatOpenAIå®ä¾‹
    """
    return LLMFactory.create_deepseek_llm(**kwargs)

def get_llm(provider: Optional[str] = None, **kwargs) -> BaseLanguageModel:
    """
    ä¾¿æ·å‡½æ•°ï¼šè·å–LLMå®ä¾‹
    
    Args:
        provider: æä¾›å•†ç±»å‹ ("openai", "deepseek", "local", "ollama")ï¼Œå¦‚æœä¸ºNoneåˆ™è‡ªåŠ¨é€‰æ‹©
        **kwargs: å…¶ä»–å‚æ•°
        
    Returns:
        LLMå®ä¾‹
    """
    if provider is None:
        return LLMFactory.auto_create_llm()
    else:
        return LLMFactory.create_llm(provider, **kwargs)

if __name__ == "__main__":
    # æµ‹è¯•LLMå·¥å‚
    print("=== LLMå·¥å‚æµ‹è¯• ===")
    
    try:
        # è‡ªåŠ¨åˆ›å»ºLLM
        llm = LLMFactory.auto_create_llm()
        print(f"âœ… æˆåŠŸåˆ›å»ºLLM: {type(llm).__name__}")
        
        # æµ‹è¯•ç®€å•å¯¹è¯
        from langchain_core.messages import HumanMessage
        response = llm.invoke([HumanMessage(content="ä½ å¥½ï¼Œè¯·ç®€å•ä»‹ç»ä¸€ä¸‹è‡ªå·±")])
        print(f"ğŸ¤– æ¨¡å‹å“åº”: {response.content[:100]}...")
        
    except Exception as e:
        print(f"âŒ LLMåˆ›å»ºå¤±è´¥: {e}")
        print("\nè¯·æ£€æŸ¥é…ç½®æ–‡ä»¶æˆ–ç¯å¢ƒå˜é‡è®¾ç½®")
    
    # æ˜¾ç¤ºé…ç½®ä¿¡æ¯
    print("\n=== å½“å‰é…ç½® ===")
    print(f"OpenAI API Key: {'å·²è®¾ç½®' if config.openai_api_key else 'æœªè®¾ç½®'}")
    print(f"OpenAI Base URL: {config.openai_base_url}")
    print(f"Local Base URL: {config.local_base_url}")
    print(f"Temperature: {config.temperature}")