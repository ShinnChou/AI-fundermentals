# ICMS 软件定义存储栈：从块存储到 AI 原生 KV 缓存服务的演进

在传统的 AI 推理架构中，KV Cache 的管理往往面临“重新计算”或“有损压缩”的权衡，且现有的存储 OEM 方案（如传统的可靠性与压缩服务）并不直接支持 KV Cache 的语义逻辑。**NVIDIA 推理上下文内存存储 (ICMS)** 通过全新的软件定义存储栈，彻底改变了这一现状。

## 1. 从“通用存储”到“KV 缓存服务”的范式转移

在 ICMS 问世前，基于 DOCA 的 NIXL 服务主要聚焦于传统存储管理，缺乏对 KV Cache 的原生支持。ICMS 的核心突破在于利用 **DOCA、Dynamo 和 NIXL** 构建了一套专为推理上下文优化的服务架构：

- **NVIDIA Dynamo (KVBM)**：作为 **KV 块管理器 (KVBM)** 运行在计算侧，负责逻辑上的 KV 数据块调度与管理。
- **NIXL Key-Value API 插件**：这是 ICMS 的核心接口层。它通过 **DOCA Key-Value NVMe API** 将存储节点的上下文数据直接暴露给 AI 计算节点，打破了传统文件系统的层级限制。
- **DOCA 库与 KV Cache Service**：运行在 **BlueField-4 (BF4)** DPU 上，提供硬件加速的 KV 缓存访问。BF4 不仅在计算节点运行，也在存储节点作为核心处理单元。

## 2. 实现 Pod 级“无状态应用”的关键技术

ICMS 软件栈的核心目标是让 **AI 应用程序保持无状态 (Stateless)**。

- **存储与计算解耦**：通过 Spectrum-X 以太网连接，计算节点可以通过标准的 **KV NVMe API** 动态拉取或存放 KV Cache。
- **Pod 级共享**：KV Cache 不再被物理隔离在单个 GPU 的 **HBM (G1 层)** 或 **本地 SSD (G3 层)** 中，而是在 Pod 级别成为可伸缩、可共享的资源池。这意味着推理任务可以在 Pod 内的不同节点间自由迁移，而无需昂贵的上下文重算。

## 3. 安全性与硬件加速的隔离

在大规模集群（如包含 1152 颗 GPU 的 **Vera Rubin SuperPOD**）中，多租户的安全隔离至关重要。ICMS 软件栈在底层实现了：

- **受保护的访问**：利用 BF4 的硬件特性，实现 **安全且隔离的 KV Cache 访问**。
- **硬件加速安全**：提供硬件级的 **数据完整性校验与加密**，在确保低延迟访问的同时，防止跨任务的数据泄露。

## 4. 效能提升

相比传统方案，这套软件栈配合 AI 原生硬件，在处理高熵数据时能提供 **5 倍的能源效率提升**，并显著优化了 **每秒事务数 (TPS)** 和 **首字延迟 (TTFT)**。
