# ç¬¬äºŒå¤©ï¼šLangGraphæ¡†æ¶æ·±åº¦åº”ç”¨

## å­¦ä¹ ç›®æ ‡

1. LangGraphåŸºç¡€ä¸æ ¸å¿ƒæ¦‚å¿µ
   - ç†è§£LangGraphçš„è®¾è®¡ç†å¿µå’Œæ ¸å¿ƒæ¦‚å¿µ
   - æŒæ¡å›¾ç»“æ„åœ¨å¤šæ™ºèƒ½ä½“ç³»ç»Ÿä¸­çš„åº”ç”¨
   - å­¦ä¼šåˆ›å»ºåŸºç¡€çš„LangGraphå·¥ä½œæµ

2. é«˜çº§å·¥ä½œæµæ„å»ºä¸æ¨¡å¼
   - æŒæ¡å¤æ‚å·¥ä½œæµçš„è®¾è®¡æ¨¡å¼
   - å­¦ä¹ å¹¶è¡Œå¤„ç†å’Œå¾ªç¯æ§åˆ¶
   - ç†è§£å·¥ä½œæµçš„æ€§èƒ½ä¼˜åŒ–æŠ€å·§

---

## å‚è€ƒé¡¹ç›®

**ğŸ’¡ å®é™…ä»£ç å‚è€ƒ**ï¼šå®Œæ•´çš„LangGraphå·¥ä½œæµå¼•æ“å®ç°å¯å‚è€ƒé¡¹ç›®ä¸­çš„ä»¥ä¸‹æ–‡ä»¶ï¼š

- `langgraph_workflow.py` - ä¼ä¸šçº§å·¥ä½œæµç®¡ç†å¼•æ“
- `multi_agent_system/src/workflows/` - å·¥ä½œæµæ¨¡æ¿å’Œå®ç°
- `multi_agent_system/src/core/state_manager.py` - çŠ¶æ€ç®¡ç†æœºåˆ¶
- `customer_service_system.py` - å®¢æœç³»ç»Ÿå·¥ä½œæµæ¡ˆä¾‹
- `monitoring/performance_monitor.py` - å·¥ä½œæµæ€§èƒ½ç›‘æ§

**ä»£ç å¼•ç”¨**: å®Œæ•´çš„ä¼ä¸šçº§å·¥ä½œæµå¼•æ“å®ç°è¯·å‚è€ƒ `multi_agent_system/src/workflow/langgraph_workflow.py`

ä¼ä¸šçº§LangGraphå·¥ä½œæµå¼•æ“çš„æ ¸å¿ƒç‰¹æ€§ï¼š

**æ ¸å¿ƒç»„ä»¶**ï¼š

- `WorkflowStatus`: å·¥ä½œæµçŠ¶æ€ç®¡ç†ï¼ˆå¾…å¤„ç†ã€è¿è¡Œä¸­ã€å·²å®Œæˆã€å¤±è´¥ã€æš‚åœï¼‰
- `WorkflowMetrics`: æ€§èƒ½æŒ‡æ ‡ç›‘æ§ï¼ˆæ‰§è¡Œæ—¶é—´ã€å†…å­˜ä½¿ç”¨ã€æˆåŠŸç‡ç­‰ï¼‰
- `EnterpriseWorkflowEngine`: ä¸»å¼•æ“ç±»ï¼Œæ”¯æŒå¹¶å‘å·¥ä½œæµç®¡ç†

**ä¼ä¸šçº§ç‰¹æ€§**ï¼š

- å¹¶å‘å·¥ä½œæµé™åˆ¶å’Œèµ„æºç®¡ç†
- å®æ—¶æ€§èƒ½æŒ‡æ ‡æ”¶é›†å’Œç›‘æ§
- çŠ¶æ€æŒä¹…åŒ–å’Œæ¢å¤æœºåˆ¶
- å¼‚å¸¸å¤„ç†å’Œè‡ªåŠ¨æ¸…ç†

---

## 1. LangGraphåŸºç¡€ä¸æ ¸å¿ƒæ¦‚å¿µï¼ˆ2å­¦æ—¶ï¼‰

### 1.1 LangGraphæ¡†æ¶æ¦‚è¿°

LangGraphæ˜¯LangChainç”Ÿæ€ç³»ç»Ÿä¸­ä¸“é—¨ç”¨äºæ„å»ºå¤šæ™ºèƒ½ä½“å·¥ä½œæµçš„æ¡†æ¶ï¼Œå®ƒå°†å¤æ‚çš„æ™ºèƒ½ä½“äº¤äº’å»ºæ¨¡ä¸ºæœ‰å‘å›¾ç»“æ„ï¼Œæ¯ä¸ªèŠ‚ç‚¹ä»£è¡¨ä¸€ä¸ªæ™ºèƒ½ä½“æˆ–å¤„ç†æ­¥éª¤ï¼Œè¾¹ä»£è¡¨æ•°æ®æµå’Œæ§åˆ¶æµã€‚

> **ğŸ’¡ å®é™…ä»£ç å‚è€ƒ**ï¼šå®Œæ•´çš„LangGraphå·¥ä½œæµå¼•æ“å®ç°å¯å‚è€ƒé¡¹ç›®ä¸­çš„ `langgraph_workflow.py` æ–‡ä»¶ï¼Œè¯¥æ–‡ä»¶æä¾›äº†ä¼ä¸šçº§çš„å·¥ä½œæµç®¡ç†ã€çŠ¶æ€éªŒè¯å’Œæ€§èƒ½ç›‘æ§åŠŸèƒ½ã€‚

#### 1.1.1 æ ¸å¿ƒè®¾è®¡ç†å¿µ

| è®¾è®¡åŸåˆ™ | å…·ä½“ä½“ç° | æŠ€æœ¯ä¼˜åŠ¿ | åº”ç”¨ä»·å€¼ |
|---------|---------|---------|---------|
| **å›¾ç»“æ„å»ºæ¨¡** | èŠ‚ç‚¹-è¾¹æ¨¡å‹è¡¨ç¤ºå·¥ä½œæµ | ç›´è§‚å¯è§†åŒ–ã€æ˜“äºç†è§£ | å¤æ‚æµç¨‹ç®¡ç†ã€è°ƒè¯•ä¾¿åˆ© |
| **çŠ¶æ€ç®¡ç†** | é›†ä¸­å¼çŠ¶æ€å­˜å‚¨ä¸æ›´æ–° | ä¸€è‡´æ€§ä¿è¯ã€å¹¶å‘å®‰å…¨ | å¤šæ™ºèƒ½ä½“åä½œã€æ•°æ®å…±äº« |
| **æµç¨‹æ§åˆ¶** | æ¡ä»¶åˆ†æ”¯ã€å¾ªç¯ã€å¹¶è¡Œ | çµæ´»çš„æ‰§è¡Œé€»è¾‘ | åŠ¨æ€å†³ç­–ã€è‡ªé€‚åº”æµç¨‹ |
| **å¯æ‰©å±•æ€§** | æ¨¡å—åŒ–èŠ‚ç‚¹ã€æ’ä»¶æœºåˆ¶ | ç»„ä»¶å¤ç”¨ã€ç³»ç»Ÿæ‰©å±• | å¿«é€Ÿå¼€å‘ã€ç»´æŠ¤ä¾¿åˆ© |
| **é”™è¯¯æ¢å¤** | æ™ºèƒ½é”™è¯¯æ£€æµ‹ä¸æ¢å¤ | è‡ªåŠ¨é‡è¯•ã€çŠ¶æ€å›æ»š | ç³»ç»Ÿç¨³å®šæ€§ã€å®¹é”™èƒ½åŠ› |

#### 1.1.2 ä¼ä¸šçº§ç‰¹æ€§å¯¹æ¯”

| å¯¹æ¯”ç»´åº¦ | LangGraph | ä¼ ç»Ÿå·¥ä½œæµå¼•æ“ | ä¼ä¸šçº§å¢å¼º |
|---------|-----------|---------------|------------|
| **AIé›†æˆ** | åŸç”Ÿæ”¯æŒLLMå’ŒAIå·¥å…· | éœ€è¦é¢å¤–é›†æˆ | æ™ºèƒ½å†³ç­–ã€è‡ªé€‚åº”ä¼˜åŒ– |
| **çŠ¶æ€ç®¡ç†** | æ™ºèƒ½çŠ¶æ€æ¨ç† | é™æ€çŠ¶æ€å®šä¹‰ | çŠ¶æ€éªŒè¯ã€ä¸€è‡´æ€§ä¿è¯ |
| **é”™è¯¯å¤„ç†** | AIé©±åŠ¨çš„é”™è¯¯æ¢å¤ | é¢„å®šä¹‰é”™è¯¯å¤„ç† | æ™ºèƒ½é‡è¯•ã€è‡ªåŠ¨æ¢å¤ |
| **æ€§èƒ½ç›‘æ§** | å®æ—¶æ€§èƒ½è¿½è¸ª | åŸºç¡€æ—¥å¿—è®°å½• | å…¨é“¾è·¯ç›‘æ§ã€æ€§èƒ½ä¼˜åŒ– |
| **å­¦ä¹ èƒ½åŠ›** | æ”¯æŒåœ¨çº¿å­¦ä¹ ä¼˜åŒ– | é™æ€æµç¨‹å®šä¹‰ | æŒç»­æ”¹è¿›ã€æ¨¡å¼è¯†åˆ« |

### 1.2 æ ¸å¿ƒæ¦‚å¿µæ·±åº¦è§£æ

#### 1.2.1 å›¾ï¼ˆGraphï¼‰ç»“æ„

```python
from langgraph.graph import StateGraph, END
from typing import TypedDict, Annotated, List
import operator

# å®šä¹‰çŠ¶æ€ç»“æ„
class AgentState(TypedDict):
    messages: Annotated[List[str], operator.add]
    current_agent: str
    task_status: str
    shared_context: dict

# åˆ›å»ºçŠ¶æ€å›¾
workflow = StateGraph(AgentState)

# æ·»åŠ èŠ‚ç‚¹
workflow.add_node("research_agent", research_node)
workflow.add_node("analysis_agent", analysis_node)
workflow.add_node("decision_agent", decision_node)

# å®šä¹‰è¾¹å’Œæ¡ä»¶
workflow.add_edge("research_agent", "analysis_agent")
workflow.add_conditional_edges(
    "analysis_agent",
    decide_next_step,
    {
        "continue": "decision_agent",
        "need_more_data": "research_agent",
        "complete": END
    }
)

# è®¾ç½®å…¥å£ç‚¹
workflow.set_entry_point("research_agent")

# ç¼–è¯‘å›¾
app = workflow.compile()
```

#### 1.2.2 èŠ‚ç‚¹ï¼ˆNodeï¼‰ç±»å‹ä¸å®ç°

| èŠ‚ç‚¹ç±»å‹ | åŠŸèƒ½æè¿° | å®ç°æ–¹å¼ | ä½¿ç”¨åœºæ™¯ |
|---------|---------|---------|---------|
| **æ™ºèƒ½ä½“èŠ‚ç‚¹** | å°è£…å•ä¸ªæ™ºèƒ½ä½“çš„å¤„ç†é€»è¾‘ | LLM + å·¥å…·è°ƒç”¨ | ä¸“ä¸šä»»åŠ¡å¤„ç† |
| **å·¥å…·èŠ‚ç‚¹** | æ‰§è¡Œç‰¹å®šå·¥å…·æˆ–APIè°ƒç”¨ | å‡½æ•°å°è£… | å¤–éƒ¨ç³»ç»Ÿé›†æˆ |
| **å†³ç­–èŠ‚ç‚¹** | åŸºäºæ¡ä»¶è¿›è¡Œè·¯å¾„é€‰æ‹© | æ¡ä»¶é€»è¾‘ | æµç¨‹æ§åˆ¶ |
| **èšåˆèŠ‚ç‚¹** | åˆå¹¶å¤šä¸ªè¾“å…¥çš„ç»“æœ | æ•°æ®åˆå¹¶ç®—æ³• | ç»“æœæ•´åˆ |

```python
# æ™ºèƒ½ä½“èŠ‚ç‚¹å®ç°ç¤ºä¾‹
async def research_agent_node(state: AgentState) -> AgentState:
    """ç ”ç©¶æ™ºèƒ½ä½“èŠ‚ç‚¹"""
    
    # è·å–å½“å‰çŠ¶æ€
    messages = state["messages"]
    context = state["shared_context"]
    
    # æ„å»ºæç¤ºè¯
    prompt = f"""
    ä½œä¸ºç ”ç©¶æ™ºèƒ½ä½“ï¼Œè¯·åŸºäºä»¥ä¸‹ä¿¡æ¯è¿›è¡Œæ·±åº¦ç ”ç©¶ï¼š
    å†å²æ¶ˆæ¯ï¼š{messages}
    ä¸Šä¸‹æ–‡ï¼š{context}
    
    è¯·æä¾›è¯¦ç»†çš„ç ”ç©¶æŠ¥å‘Šã€‚
    """
    
    # è°ƒç”¨LLM
    llm = ChatOpenAI(model="gpt-4")
    response = await llm.ainvoke(prompt)
    
    # æ›´æ–°çŠ¶æ€
    return {
        "messages": [f"Research Agent: {response.content}"],
        "current_agent": "research_agent",
        "task_status": "research_completed",
        "shared_context": {
            **context,
            "research_data": response.content
        }
    }

# æ¡ä»¶å†³ç­–èŠ‚ç‚¹
def decide_next_step(state: AgentState) -> str:
    """å†³ç­–ä¸‹ä¸€æ­¥æ‰§è¡Œè·¯å¾„"""
    
    task_status = state["task_status"]
    context = state["shared_context"]
    
    # æ£€æŸ¥ç ”ç©¶æ•°æ®è´¨é‡
    research_data = context.get("research_data", "")
    
    if len(research_data) < 100:
        return "need_more_data"
    elif "analysis_required" in research_data.lower():
        return "continue"
    else:
        return "complete"
```

#### 1.2.3 çŠ¶æ€ç®¡ç†æœºåˆ¶

LangGraphçš„çŠ¶æ€ç®¡ç†æ˜¯å…¶æ ¸å¿ƒç‰¹æ€§ä¹‹ä¸€ï¼Œæ”¯æŒå¤šç§çŠ¶æ€æ›´æ–°æ¨¡å¼ï¼š

```python
from typing import Annotated
import operator

# çŠ¶æ€å®šä¹‰ç¤ºä¾‹
class MultiAgentState(TypedDict):
    # ç´¯åŠ æ¨¡å¼ï¼šæ–°æ¶ˆæ¯è¿½åŠ åˆ°åˆ—è¡¨
    messages: Annotated[List[str], operator.add]
    
    # æ›¿æ¢æ¨¡å¼ï¼šæ–°å€¼ç›´æ¥æ›¿æ¢æ—§å€¼
    current_step: str
    
    # åˆå¹¶æ¨¡å¼ï¼šå­—å…¸åˆå¹¶
    agent_outputs: Annotated[dict, lambda x, y: {**x, **y}]
    
    # è‡ªå®šä¹‰æ›´æ–°å‡½æ•°
    confidence_scores: Annotated[List[float], lambda x, y: x + y if x else y]

# çŠ¶æ€æ›´æ–°ç¤ºä¾‹
def update_confidence(existing: List[float], new: List[float]) -> List[float]:
    """è‡ªå®šä¹‰ç½®ä¿¡åº¦æ›´æ–°é€»è¾‘"""
    if not existing:
        return new
    
    # è®¡ç®—åŠ æƒå¹³å‡
    combined = []
    for i in range(min(len(existing), len(new))):
        weighted_score = (existing[i] * 0.7 + new[i] * 0.3)
        combined.append(weighted_score)
    
    return combined

# çŠ¶æ€è®¿é—®å’Œä¿®æ”¹
class StateManager:
    """çŠ¶æ€ç®¡ç†å™¨"""
    
    def __init__(self, initial_state: MultiAgentState):
        self.state = initial_state
        self.history = [initial_state.copy()]
    
    def update_state(self, updates: dict) -> MultiAgentState:
        """æ›´æ–°çŠ¶æ€"""
        new_state = self.state.copy()
        
        for key, value in updates.items():
            if key in new_state:
                # æ ¹æ®æ³¨è§£ç±»å‹è¿›è¡Œæ›´æ–°
                annotation = MultiAgentState.__annotations__.get(key)
                if hasattr(annotation, '__metadata__'):
                    update_func = annotation.__metadata__[0]
                    new_state[key] = update_func(new_state[key], value)
                else:
                    new_state[key] = value
        
        self.state = new_state
        self.history.append(new_state.copy())
        return new_state
    
    def rollback(self, steps: int = 1) -> MultiAgentState:
        """çŠ¶æ€å›æ»š"""
        if len(self.history) > steps:
            self.state = self.history[-(steps + 1)].copy()
            self.history = self.history[:-(steps)]
        return self.state
```

### 1.4 åŸºç¡€å·¥ä½œæµæ„å»º

#### 1.4.1 ç®€å•çº¿æ€§å·¥ä½œæµ

```python
from langgraph.graph import StateGraph, END
from langgraph.prebuilt import ToolExecutor
from langchain_openai import ChatOpenAI

# å®šä¹‰ç®€å•çš„å®¢æœå·¥ä½œæµ
class CustomerServiceState(TypedDict):
    customer_query: str
    intent: str
    response: str
    satisfaction_score: float

def intent_recognition_node(state: CustomerServiceState) -> CustomerServiceState:
    """æ„å›¾è¯†åˆ«èŠ‚ç‚¹"""
    query = state["customer_query"]
    
    # ä½¿ç”¨LLMè¿›è¡Œæ„å›¾è¯†åˆ«
    llm = ChatOpenAI(model="gpt-3.5-turbo")
    prompt = f"åˆ†æä»¥ä¸‹å®¢æˆ·æŸ¥è¯¢çš„æ„å›¾ï¼š{query}\nè¿”å›æ„å›¾ç±»åˆ«ï¼šæŠ€æœ¯æ”¯æŒ/è´¦å•æŸ¥è¯¢/äº§å“å’¨è¯¢/æŠ•è¯‰å»ºè®®"
    
    response = llm.invoke(prompt)
    intent = response.content.strip()
    
    return {
        **state,
        "intent": intent
    }

def response_generation_node(state: CustomerServiceState) -> CustomerServiceState:
    """å“åº”ç”ŸæˆèŠ‚ç‚¹"""
    query = state["customer_query"]
    intent = state["intent"]
    
    llm = ChatOpenAI(model="gpt-4")
    prompt = f"""
    å®¢æˆ·æŸ¥è¯¢ï¼š{query}
    è¯†åˆ«æ„å›¾ï¼š{intent}
    
    è¯·ç”Ÿæˆä¸“ä¸šã€å‹å¥½çš„å®¢æœå“åº”ã€‚
    """
    
    response = llm.invoke(prompt)
    
    return {
        **state,
        "response": response.content
    }

def satisfaction_evaluation_node(state: CustomerServiceState) -> CustomerServiceState:
    """æ»¡æ„åº¦è¯„ä¼°èŠ‚ç‚¹"""
    query = state["customer_query"]
    response = state["response"]
    
    # ç®€åŒ–çš„æ»¡æ„åº¦è¯„ä¼°
    llm = ChatOpenAI(model="gpt-3.5-turbo")
    prompt = f"""
    è¯„ä¼°ä»¥ä¸‹å®¢æœå¯¹è¯çš„æ»¡æ„åº¦ï¼ˆ0-10åˆ†ï¼‰ï¼š
    å®¢æˆ·é—®é¢˜ï¼š{query}
    å®¢æœå›å¤ï¼š{response}
    
    åªè¿”å›æ•°å­—åˆ†æ•°ã€‚
    """
    
    score_response = llm.invoke(prompt)
    try:
        score = float(score_response.content.strip())
    except:
        score = 7.0  # é»˜è®¤åˆ†æ•°
    
    return {
        **state,
        "satisfaction_score": score
    }

# æ„å»ºå·¥ä½œæµ
def create_customer_service_workflow():
    """åˆ›å»ºå®¢æœå·¥ä½œæµ"""
    
    workflow = StateGraph(CustomerServiceState)
    
    # æ·»åŠ èŠ‚ç‚¹
    workflow.add_node("intent_recognition", intent_recognition_node)
    workflow.add_node("response_generation", response_generation_node)
    workflow.add_node("satisfaction_evaluation", satisfaction_evaluation_node)
    
    # å®šä¹‰æ‰§è¡Œé¡ºåº
    workflow.add_edge("intent_recognition", "response_generation")
    workflow.add_edge("response_generation", "satisfaction_evaluation")
    workflow.add_edge("satisfaction_evaluation", END)
    
    # è®¾ç½®å…¥å£ç‚¹
    workflow.set_entry_point("intent_recognition")
    
    return workflow.compile()

# ä½¿ç”¨ç¤ºä¾‹
async def run_customer_service():
    """è¿è¡Œå®¢æœå·¥ä½œæµ"""
    
    app = create_customer_service_workflow()
    
    # åˆå§‹çŠ¶æ€
    initial_state = {
        "customer_query": "æˆ‘çš„è´¦å•æœ‰é—®é¢˜ï¼Œä¸ºä»€ä¹ˆè¿™ä¸ªæœˆè´¹ç”¨è¿™ä¹ˆé«˜ï¼Ÿ",
        "intent": "",
        "response": "",
        "satisfaction_score": 0.0
    }
    
    # æ‰§è¡Œå·¥ä½œæµ
    result = await app.ainvoke(initial_state)
    
    print(f"å®¢æˆ·æŸ¥è¯¢ï¼š{result['customer_query']}")
    print(f"è¯†åˆ«æ„å›¾ï¼š{result['intent']}")
    print(f"å®¢æœå›å¤ï¼š{result['response']}")
    print(f"æ»¡æ„åº¦è¯„åˆ†ï¼š{result['satisfaction_score']}")
    
    return result
```

#### 1.4.2 æ¡ä»¶åˆ†æ”¯å·¥ä½œæµ

```python
# å¤æ‚çš„æ¡ä»¶åˆ†æ”¯å·¥ä½œæµ
class ComplexTaskState(TypedDict):
    task_description: str
    complexity_level: str
    assigned_agents: List[str]
    results: dict
    final_output: str

def task_analysis_node(state: ComplexTaskState) -> ComplexTaskState:
    """ä»»åŠ¡åˆ†æèŠ‚ç‚¹"""
    description = state["task_description"]
    
    # åˆ†æä»»åŠ¡å¤æ‚åº¦
    llm = ChatOpenAI(model="gpt-4")
    prompt = f"""
    åˆ†æä»¥ä¸‹ä»»åŠ¡çš„å¤æ‚åº¦ï¼š{description}
    
    è¿”å›å¤æ‚åº¦çº§åˆ«ï¼šç®€å•/ä¸­ç­‰/å¤æ‚/æå¤æ‚
    """
    
    response = llm.invoke(prompt)
    complexity = response.content.strip()
    
    return {
        **state,
        "complexity_level": complexity
    }

def simple_task_node(state: ComplexTaskState) -> ComplexTaskState:
    """ç®€å•ä»»åŠ¡å¤„ç†èŠ‚ç‚¹"""
    description = state["task_description"]
    
    llm = ChatOpenAI(model="gpt-3.5-turbo")
    response = llm.invoke(f"ç›´æ¥å¤„ç†è¿™ä¸ªç®€å•ä»»åŠ¡ï¼š{description}")
    
    return {
        **state,
        "assigned_agents": ["general_agent"],
        "results": {"general_agent": response.content},
        "final_output": response.content
    }

def complex_task_node(state: ComplexTaskState) -> ComplexTaskState:
    """å¤æ‚ä»»åŠ¡å¤„ç†èŠ‚ç‚¹"""
    description = state["task_description"]
    
    # åˆ†é…ç»™å¤šä¸ªä¸“ä¸šæ™ºèƒ½ä½“
    agents = ["research_agent", "analysis_agent", "synthesis_agent"]
    results = {}
    
    for agent in agents:
        llm = ChatOpenAI(model="gpt-4")
        prompt = f"ä½œä¸º{agent}ï¼Œå¤„ç†ä»»åŠ¡çš„ç›¸å…³éƒ¨åˆ†ï¼š{description}"
        response = llm.invoke(prompt)
        results[agent] = response.content
    
    return {
        **state,
        "assigned_agents": agents,
        "results": results
    }

def result_synthesis_node(state: ComplexTaskState) -> ComplexTaskState:
    """ç»“æœç»¼åˆèŠ‚ç‚¹"""
    results = state["results"]
    
    # ç»¼åˆæ‰€æœ‰æ™ºèƒ½ä½“çš„ç»“æœ
    combined_results = "\n".join([f"{agent}: {result}" for agent, result in results.items()])
    
    llm = ChatOpenAI(model="gpt-4")
    prompt = f"""
    ç»¼åˆä»¥ä¸‹æ™ºèƒ½ä½“çš„å¤„ç†ç»“æœï¼Œç”Ÿæˆæœ€ç»ˆè¾“å‡ºï¼š
    {combined_results}
    """
    
    final_output = llm.invoke(prompt)
    
    return {
        **state,
        "final_output": final_output.content
    }

def route_by_complexity(state: ComplexTaskState) -> str:
    """æ ¹æ®å¤æ‚åº¦è·¯ç”±"""
    complexity = state["complexity_level"]
    
    if complexity in ["ç®€å•"]:
        return "simple_task"
    else:
        return "complex_task"

def check_need_synthesis(state: ComplexTaskState) -> str:
    """æ£€æŸ¥æ˜¯å¦éœ€è¦ç»“æœç»¼åˆ"""
    agents = state["assigned_agents"]
    
    if len(agents) > 1:
        return "synthesis"
    else:
        return "end"

# æ„å»ºæ¡ä»¶åˆ†æ”¯å·¥ä½œæµ
def create_adaptive_workflow():
    """åˆ›å»ºè‡ªé€‚åº”å·¥ä½œæµ"""
    
    workflow = StateGraph(ComplexTaskState)
    
    # æ·»åŠ èŠ‚ç‚¹
    workflow.add_node("task_analysis", task_analysis_node)
    workflow.add_node("simple_task", simple_task_node)
    workflow.add_node("complex_task", complex_task_node)
    workflow.add_node("result_synthesis", result_synthesis_node)
    
    # è®¾ç½®æ¡ä»¶åˆ†æ”¯
    workflow.add_conditional_edges(
        "task_analysis",
        route_by_complexity,
        {
            "simple_task": "simple_task",
            "complex_task": "complex_task"
        }
    )
    
    workflow.add_conditional_edges(
        "simple_task",
        check_need_synthesis,
        {
            "synthesis": "result_synthesis",
            "end": END
        }
    )
    
    workflow.add_conditional_edges(
        "complex_task",
        check_need_synthesis,
        {
            "synthesis": "result_synthesis",
            "end": END
        }
    )
    
    workflow.add_edge("result_synthesis", END)
    workflow.set_entry_point("task_analysis")
    
    return workflow.compile()
```

### 1.5 å®è·µç¯èŠ‚

#### 1.5.1 ç»ƒä¹ 1ï¼šæ„å»ºç®€å•çš„å¤šæ™ºèƒ½ä½“å¯¹è¯ç³»ç»Ÿ

```python
# å¤šæ™ºèƒ½ä½“å¯¹è¯ç³»ç»Ÿ
class ConversationState(TypedDict):
    user_input: str
    conversation_history: Annotated[List[str], operator.add]
    current_speaker: str
    topic: str
    sentiment: str

def moderator_node(state: ConversationState) -> ConversationState:
    """ä¸»æŒäººèŠ‚ç‚¹"""
    user_input = state["user_input"]
    history = state["conversation_history"]
    
    # åˆ†æè¯é¢˜å’Œæƒ…æ„Ÿ
    llm = ChatOpenAI(model="gpt-3.5-turbo")
    
    topic_prompt = f"åˆ†æå¯¹è¯ä¸»é¢˜ï¼š{user_input}\nå†å²ï¼š{history[-3:]}\nè¿”å›ä¸»é¢˜å…³é”®è¯"
    topic_response = llm.invoke(topic_prompt)
    
    sentiment_prompt = f"åˆ†ææƒ…æ„Ÿå€¾å‘ï¼š{user_input}\nè¿”å›ï¼šç§¯æ/ä¸­æ€§/æ¶ˆæ"
    sentiment_response = llm.invoke(sentiment_prompt)
    
    return {
        **state,
        "topic": topic_response.content.strip(),
        "sentiment": sentiment_response.content.strip(),
        "current_speaker": "moderator"
    }

def expert_agent_node(state: ConversationState) -> ConversationState:
    """ä¸“å®¶æ™ºèƒ½ä½“èŠ‚ç‚¹"""
    user_input = state["user_input"]
    topic = state["topic"]
    
    llm = ChatOpenAI(model="gpt-4")
    prompt = f"""
    ä½œä¸º{topic}é¢†åŸŸçš„ä¸“å®¶ï¼Œå›åº”ç”¨æˆ·ï¼š{user_input}
    æä¾›ä¸“ä¸šã€è¯¦ç»†çš„å›ç­”ã€‚
    """
    
    response = llm.invoke(prompt)
    
    return {
        **state,
        "conversation_history": [f"Expert: {response.content}"],
        "current_speaker": "expert"
    }

def empathy_agent_node(state: ConversationState) -> ConversationState:
    """å…±æƒ…æ™ºèƒ½ä½“èŠ‚ç‚¹"""
    user_input = state["user_input"]
    sentiment = state["sentiment"]
    
    llm = ChatOpenAI(model="gpt-3.5-turbo")
    prompt = f"""
    ç”¨æˆ·æƒ…æ„ŸçŠ¶æ€ï¼š{sentiment}
    ç”¨æˆ·è¾“å…¥ï¼š{user_input}
    
    æä¾›å…±æƒ…å’Œæƒ…æ„Ÿæ”¯æŒçš„å›åº”ã€‚
    """
    
    response = llm.invoke(prompt)
    
    return {
        **state,
        "conversation_history": [f"Empathy Agent: {response.content}"],
        "current_speaker": "empathy"
    }

def route_conversation(state: ConversationState) -> str:
    """å¯¹è¯è·¯ç”±"""
    sentiment = state["sentiment"]
    topic = state["topic"]
    
    # æ ¹æ®æƒ…æ„Ÿå’Œè¯é¢˜å†³å®šè·¯ç”±
    if sentiment == "æ¶ˆæ":
        return "empathy_agent"
    elif any(keyword in topic.lower() for keyword in ["æŠ€æœ¯", "ä¸“ä¸š", "ç§‘å­¦"]):
        return "expert_agent"
    else:
        return "expert_agent"  # é»˜è®¤è·¯ç”±

# æ„å»ºå¯¹è¯å·¥ä½œæµ
def create_conversation_workflow():
    """åˆ›å»ºå¯¹è¯å·¥ä½œæµ"""
    
    workflow = StateGraph(ConversationState)
    
    workflow.add_node("moderator", moderator_node)
    workflow.add_node("expert_agent", expert_agent_node)
    workflow.add_node("empathy_agent", empathy_agent_node)
    
    workflow.add_edge("moderator", "route_conversation")
    workflow.add_conditional_edges(
        "moderator",
        route_conversation,
        {
            "expert_agent": "expert_agent",
            "empathy_agent": "empathy_agent"
        }
    )
    
    workflow.add_edge("expert_agent", END)
    workflow.add_edge("empathy_agent", END)
    workflow.set_entry_point("moderator")
    
    return workflow.compile()

# ä½¿ç”¨ç¤ºä¾‹
async def run_conversation():
    """è¿è¡Œå¯¹è¯ç³»ç»Ÿ"""
    
    app = create_conversation_workflow()
    
    test_inputs = [
        "æˆ‘æœ€è¿‘å·¥ä½œå‹åŠ›å¾ˆå¤§ï¼Œæ„Ÿè§‰å¾ˆç„¦è™‘",
        "èƒ½è§£é‡Šä¸€ä¸‹æœºå™¨å­¦ä¹ çš„åŸºæœ¬åŸç†å—ï¼Ÿ",
        "ä»Šå¤©å¤©æ°”çœŸå¥½ï¼Œå¿ƒæƒ…ä¸é”™"
    ]
    
    for user_input in test_inputs:
        initial_state = {
            "user_input": user_input,
            "conversation_history": [],
            "current_speaker": "",
            "topic": "",
            "sentiment": ""
        }
        
        result = await app.ainvoke(initial_state)
        print(f"ç”¨æˆ·ï¼š{user_input}")
        print(f"ç³»ç»Ÿå›åº”ï¼š{result['conversation_history'][-1]}")
        print(f"è¯é¢˜ï¼š{result['topic']}, æƒ…æ„Ÿï¼š{result['sentiment']}")
        print("-" * 50)
```

---

## 2. é«˜çº§å·¥ä½œæµæ„å»ºä¸æ¨¡å¼ï¼ˆ3å­¦æ—¶ï¼‰

### 2.1 å¹¶è¡Œå¤„ç†æ¨¡å¼

åœ¨å¤šæ™ºèƒ½ä½“ç³»ç»Ÿä¸­ï¼Œå¹¶è¡Œå¤„ç†æ˜¯æé«˜æ•ˆç‡çš„å…³é”®æŠ€æœ¯ã€‚LangGraphæä¾›äº†å¤šç§å¹¶è¡Œå¤„ç†æ¨¡å¼ã€‚

#### 2.1.1 å¹¶è¡Œæ‰§è¡Œæ¨¡å¼å¯¹æ¯”

| å¹¶è¡Œæ¨¡å¼ | é€‚ç”¨åœºæ™¯ | ä¼˜åŠ¿ | æ³¨æ„äº‹é¡¹ |
|---------|---------|------|----------|
| **ç‹¬ç«‹å¹¶è¡Œ** | æ— ä¾èµ–çš„ç‹¬ç«‹ä»»åŠ¡ | æœ€å¤§åŒ–å¹¶è¡Œåº¦ | èµ„æºç«äº‰ |
| **ç®¡é“å¹¶è¡Œ** | æµæ°´çº¿å¼å¤„ç† | æŒç»­åå | åŒæ­¥å¤æ‚ |
| **åˆ†æ²»å¹¶è¡Œ** | å¯åˆ†è§£çš„å¤§ä»»åŠ¡ | è´Ÿè½½å‡è¡¡ | ç»“æœåˆå¹¶ |
| **æ¡ä»¶å¹¶è¡Œ** | åŸºäºæ¡ä»¶çš„é€‰æ‹©æ€§å¹¶è¡Œ | èµ„æºä¼˜åŒ– | é€»è¾‘å¤æ‚ |

#### 2.1.2 ç‹¬ç«‹å¹¶è¡Œå¤„ç†å®ç°

```python
from langgraph.graph import StateGraph, END
from langgraph.pregel import Pregel
import asyncio
from typing import Dict, List

class ParallelAnalysisState(TypedDict):
    input_data: str
    market_analysis: str
    technical_analysis: str
    sentiment_analysis: str
    risk_analysis: str
    final_report: str

async def market_analysis_node(state: ParallelAnalysisState) -> ParallelAnalysisState:
    """å¸‚åœºåˆ†æèŠ‚ç‚¹"""
    data = state["input_data"]
    
    # æ¨¡æ‹Ÿå¸‚åœºåˆ†æå¤„ç†
    await asyncio.sleep(2)  # æ¨¡æ‹Ÿå¤„ç†æ—¶é—´
    
    llm = ChatOpenAI(model="gpt-4")
    prompt = f"è¿›è¡Œå¸‚åœºåˆ†æï¼š{data}"
    response = await llm.ainvoke(prompt)
    
    return {
        **state,
        "market_analysis": response.content
    }

async def technical_analysis_node(state: ParallelAnalysisState) -> ParallelAnalysisState:
    """æŠ€æœ¯åˆ†æèŠ‚ç‚¹"""
    data = state["input_data"]
    
    await asyncio.sleep(1.5)  # æ¨¡æ‹Ÿå¤„ç†æ—¶é—´
    
    llm = ChatOpenAI(model="gpt-4")
    prompt = f"è¿›è¡ŒæŠ€æœ¯åˆ†æï¼š{data}"
    response = await llm.ainvoke(prompt)
    
    return {
        **state,
        "technical_analysis": response.content
    }

async def sentiment_analysis_node(state: ParallelAnalysisState) -> ParallelAnalysisState:
    """æƒ…æ„Ÿåˆ†æèŠ‚ç‚¹"""
    data = state["input_data"]
    
    await asyncio.sleep(1)  # æ¨¡æ‹Ÿå¤„ç†æ—¶é—´
    
    llm = ChatOpenAI(model="gpt-3.5-turbo")
    prompt = f"è¿›è¡Œæƒ…æ„Ÿåˆ†æï¼š{data}"
    response = await llm.ainvoke(prompt)
    
    return {
        **state,
        "sentiment_analysis": response.content
    }

async def risk_analysis_node(state: ParallelAnalysisState) -> ParallelAnalysisState:
    """é£é™©åˆ†æèŠ‚ç‚¹"""
    data = state["input_data"]
    
    await asyncio.sleep(2.5)  # æ¨¡æ‹Ÿå¤„ç†æ—¶é—´
    
    llm = ChatOpenAI(model="gpt-4")
    prompt = f"è¿›è¡Œé£é™©åˆ†æï¼š{data}"
    response = await llm.ainvoke(prompt)
    
    return {
        **state,
        "risk_analysis": response.content
    }

async def synthesis_node(state: ParallelAnalysisState) -> ParallelAnalysisState:
    """ç»¼åˆåˆ†æèŠ‚ç‚¹"""
    
    # ç­‰å¾…æ‰€æœ‰åˆ†æå®Œæˆ
    analyses = {
        "å¸‚åœºåˆ†æ": state["market_analysis"],
        "æŠ€æœ¯åˆ†æ": state["technical_analysis"],
        "æƒ…æ„Ÿåˆ†æ": state["sentiment_analysis"],
        "é£é™©åˆ†æ": state["risk_analysis"]
    }
    
    # ç»¼åˆæ‰€æœ‰åˆ†æç»“æœ
    combined_analysis = "\n".join([f"{key}: {value}" for key, value in analyses.items()])
    
    llm = ChatOpenAI(model="gpt-4")
    prompt = f"""
    åŸºäºä»¥ä¸‹åˆ†æç»“æœï¼Œç”Ÿæˆç»¼åˆæŠ•èµ„å»ºè®®ï¼š
    {combined_analysis}
    
    è¯·æä¾›æ˜ç¡®çš„æŠ•èµ„å»ºè®®å’Œé£é™©æç¤ºã€‚
    """
    
    response = await llm.ainvoke(prompt)
    
    return {
        **state,
        "final_report": response.content
    }

def create_parallel_analysis_workflow():
    """åˆ›å»ºå¹¶è¡Œåˆ†æå·¥ä½œæµ"""
    
    workflow = StateGraph(ParallelAnalysisState)
    
    # æ·»åŠ å¹¶è¡Œåˆ†æèŠ‚ç‚¹
    workflow.add_node("market_analysis", market_analysis_node)
    workflow.add_node("technical_analysis", technical_analysis_node)
    workflow.add_node("sentiment_analysis", sentiment_analysis_node)
    workflow.add_node("risk_analysis", risk_analysis_node)
    workflow.add_node("synthesis", synthesis_node)
    
    # è®¾ç½®å¹¶è¡Œæ‰§è¡Œ
    # æ‰€æœ‰åˆ†æèŠ‚ç‚¹å¹¶è¡Œæ‰§è¡Œ
    workflow.set_entry_point("market_analysis")
    workflow.set_entry_point("technical_analysis")
    workflow.set_entry_point("sentiment_analysis")
    workflow.set_entry_point("risk_analysis")
    
    # æ‰€æœ‰åˆ†æå®Œæˆåè¿›è¡Œç»¼åˆ
    workflow.add_edge("market_analysis", "synthesis")
    workflow.add_edge("technical_analysis", "synthesis")
    workflow.add_edge("sentiment_analysis", "synthesis")
    workflow.add_edge("risk_analysis", "synthesis")
    
    workflow.add_edge("synthesis", END)
    
    return workflow.compile()

# ä½¿ç”¨ç¤ºä¾‹
async def run_parallel_analysis():
    """è¿è¡Œå¹¶è¡Œåˆ†æ"""
    
    app = create_parallel_analysis_workflow()
    
    initial_state = {
        "input_data": "AAPLè‚¡ç¥¨ï¼Œå½“å‰ä»·æ ¼150ç¾å…ƒï¼Œè¿‘æœŸè´¢æŠ¥æ˜¾ç¤ºè¥æ”¶å¢é•¿15%",
        "market_analysis": "",
        "technical_analysis": "",
        "sentiment_analysis": "",
        "risk_analysis": "",
        "final_report": ""
    }
    
    start_time = asyncio.get_event_loop().time()
    result = await app.ainvoke(initial_state)
    end_time = asyncio.get_event_loop().time()
    
    print(f"å¹¶è¡Œå¤„ç†å®Œæˆï¼Œè€—æ—¶ï¼š{end_time - start_time:.2f}ç§’")
    print(f"æœ€ç»ˆæŠ¥å‘Šï¼š{result['final_report']}")
    
    return result
```

#### 2.1.3 ç®¡é“å¹¶è¡Œå¤„ç†

```python
class PipelineState(TypedDict):
    raw_data: List[str]
    processed_items: Annotated[List[dict], operator.add]
    current_batch: List[str]
    batch_index: int
    total_batches: int

async def data_ingestion_node(state: PipelineState) -> PipelineState:
    """æ•°æ®æ‘„å–èŠ‚ç‚¹"""
    raw_data = state["raw_data"]
    batch_size = 5
    
    # åˆ†æ‰¹å¤„ç†
    batches = [raw_data[i:i+batch_size] for i in range(0, len(raw_data), batch_size)]
    
    return {
        **state,
        "total_batches": len(batches),
        "batch_index": 0,
        "current_batch": batches[0] if batches else []
    }

async def processing_node(state: PipelineState) -> PipelineState:
    """å¤„ç†èŠ‚ç‚¹"""
    current_batch = state["current_batch"]
    batch_index = state["batch_index"]
    
    # å¤„ç†å½“å‰æ‰¹æ¬¡
    processed_batch = []
    for item in current_batch:
        # æ¨¡æ‹Ÿå¤„ç†
        processed_item = {
            "original": item,
            "processed": f"processed_{item}",
            "batch": batch_index,
            "timestamp": datetime.now().isoformat()
        }
        processed_batch.append(processed_item)
    
    return {
        **state,
        "processed_items": processed_batch
    }

async def batch_controller_node(state: PipelineState) -> PipelineState:
    """æ‰¹æ¬¡æ§åˆ¶èŠ‚ç‚¹"""
    batch_index = state["batch_index"]
    total_batches = state["total_batches"]
    raw_data = state["raw_data"]
    batch_size = 5
    
    # å‡†å¤‡ä¸‹ä¸€æ‰¹æ¬¡
    next_batch_index = batch_index + 1
    
    if next_batch_index < total_batches:
        start_idx = next_batch_index * batch_size
        end_idx = min(start_idx + batch_size, len(raw_data))
        next_batch = raw_data[start_idx:end_idx]
        
        return {
            **state,
            "batch_index": next_batch_index,
            "current_batch": next_batch
        }
    else:
        return {
            **state,
            "current_batch": []
        }

def should_continue_pipeline(state: PipelineState) -> str:
    """åˆ¤æ–­æ˜¯å¦ç»§ç»­ç®¡é“å¤„ç†"""
    current_batch = state["current_batch"]
    
    if current_batch:
        return "continue"
    else:
        return "end"

def create_pipeline_workflow():
    """åˆ›å»ºç®¡é“å·¥ä½œæµ"""
    
    workflow = StateGraph(PipelineState)
    
    workflow.add_node("data_ingestion", data_ingestion_node)
    workflow.add_node("processing", processing_node)
    workflow.add_node("batch_controller", batch_controller_node)
    
    # è®¾ç½®ç®¡é“æµç¨‹
    workflow.add_edge("data_ingestion", "processing")
    workflow.add_edge("processing", "batch_controller")
    
    workflow.add_conditional_edges(
        "batch_controller",
        should_continue_pipeline,
        {
            "continue": "processing",
            "end": END
        }
    )
    
    workflow.set_entry_point("data_ingestion")
    
    return workflow.compile()
```

### 2.3 å¾ªç¯æ§åˆ¶ä¸è¿­ä»£ä¼˜åŒ–

å¾ªç¯æ§åˆ¶æ˜¯å¤„ç†éœ€è¦å¤šæ¬¡è¿­ä»£çš„ä»»åŠ¡çš„å…³é”®æœºåˆ¶ã€‚

#### 2.3.1 å¾ªç¯æ§åˆ¶æ¨¡å¼

| å¾ªç¯ç±»å‹ | æ§åˆ¶æ¡ä»¶ | é€‚ç”¨åœºæ™¯ | å®ç°è¦ç‚¹ |
|---------|---------|---------|----------|
| **è®¡æ•°å¾ªç¯** | å›ºå®šæ¬¡æ•° | æ‰¹å¤„ç†ä»»åŠ¡ | è®¡æ•°å™¨ç®¡ç† |
| **æ¡ä»¶å¾ªç¯** | åŠ¨æ€æ¡ä»¶ | ä¼˜åŒ–ç®—æ³• | æ”¶æ•›åˆ¤æ–­ |
| **åé¦ˆå¾ªç¯** | è´¨é‡è¯„ä¼° | è¿­ä»£æ”¹è¿› | åé¦ˆæœºåˆ¶ |
| **è‡ªé€‚åº”å¾ªç¯** | æ™ºèƒ½å†³ç­– | å¤æ‚é—®é¢˜ | å­¦ä¹ è°ƒæ•´ |

#### 2.3.2 è¿­ä»£ä¼˜åŒ–å®ç°

```python
class IterativeOptimizationState(TypedDict):
    problem_description: str
    current_solution: str
    iteration_count: int
    max_iterations: int
    quality_score: float
    improvement_threshold: float
    optimization_history: Annotated[List[dict], operator.add]

async def solution_generator_node(state: IterativeOptimizationState) -> IterativeOptimizationState:
    """è§£å†³æ–¹æ¡ˆç”ŸæˆèŠ‚ç‚¹"""
    
    problem = state["problem_description"]
    current_solution = state["current_solution"]
    iteration = state["iteration_count"]
    history = state["optimization_history"]
    
    llm = ChatOpenAI(model="gpt-4")
    
    if iteration == 0:
        # åˆå§‹è§£å†³æ–¹æ¡ˆ
        prompt = f"ä¸ºä»¥ä¸‹é—®é¢˜ç”Ÿæˆåˆå§‹è§£å†³æ–¹æ¡ˆï¼š{problem}"
    else:
        # åŸºäºå†å²æ”¹è¿›
        recent_feedback = history[-1] if history else {}
        prompt = f"""
        é—®é¢˜ï¼š{problem}
        å½“å‰è§£å†³æ–¹æ¡ˆï¼š{current_solution}
        ä¸Šæ¬¡åé¦ˆï¼š{recent_feedback.get('feedback', '')}
        
        è¯·ç”Ÿæˆæ”¹è¿›çš„è§£å†³æ–¹æ¡ˆã€‚
        """
    
    response = await llm.ainvoke(prompt)
    new_solution = response.content
    
    return {
        **state,
        "current_solution": new_solution,
        "iteration_count": iteration + 1
    }

async def quality_evaluator_node(state: IterativeOptimizationState) -> IterativeOptimizationState:
    """è´¨é‡è¯„ä¼°èŠ‚ç‚¹"""
    
    problem = state["problem_description"]
    solution = state["current_solution"]
    
    llm = ChatOpenAI(model="gpt-4")
    prompt = f"""
    è¯„ä¼°ä»¥ä¸‹è§£å†³æ–¹æ¡ˆçš„è´¨é‡ï¼ˆ0-10åˆ†ï¼‰ï¼š
    é—®é¢˜ï¼š{problem}
    è§£å†³æ–¹æ¡ˆï¼š{solution}
    
    è¯„ä¼°ç»´åº¦ï¼š
    1. å¯è¡Œæ€§ (0-10)
    2. åˆ›æ–°æ€§ (0-10)
    3. æ•ˆç‡ (0-10)
    4. å®Œæ•´æ€§ (0-10)
    
    è¿”å›æ ¼å¼ï¼š
    æ€»åˆ†ï¼šX.X
    å¯è¡Œæ€§ï¼šX.X
    åˆ›æ–°æ€§ï¼šX.X
    æ•ˆç‡ï¼šX.X
    å®Œæ•´æ€§ï¼šX.X
    åé¦ˆï¼šå…·ä½“æ”¹è¿›å»ºè®®
    """
    
    response = await llm.ainvoke(prompt)
    evaluation = response.content
    
    # è§£æè¯„åˆ†
    try:
        lines = evaluation.split('\n')
        total_score = float(lines[0].split('ï¼š')[1])
    except:
        total_score = 5.0  # é»˜è®¤åˆ†æ•°
    
    # è®°å½•ä¼˜åŒ–å†å²
    history_entry = {
        "iteration": state["iteration_count"],
        "solution": solution,
        "score": total_score,
        "evaluation": evaluation,
        "timestamp": datetime.now().isoformat()
    }
    
    return {
        **state,
        "quality_score": total_score,
        "optimization_history": [history_entry]
    }

def should_continue_optimization(state: IterativeOptimizationState) -> str:
    """åˆ¤æ–­æ˜¯å¦ç»§ç»­ä¼˜åŒ–"""
    
    iteration = state["iteration_count"]
    max_iterations = state["max_iterations"]
    quality_score = state["quality_score"]
    threshold = state["improvement_threshold"]
    history = state["optimization_history"]
    
    # æ£€æŸ¥æœ€å¤§è¿­ä»£æ¬¡æ•°
    if iteration >= max_iterations:
        return "max_iterations_reached"
    
    # æ£€æŸ¥è´¨é‡é˜ˆå€¼
    if quality_score >= 9.0:
        return "quality_threshold_reached"
    
    # æ£€æŸ¥æ”¹è¿›å¹…åº¦
    if len(history) >= 2:
        recent_scores = [entry["score"] for entry in history[-2:]]
        improvement = recent_scores[-1] - recent_scores[-2]
        
        if improvement < threshold:
            return "improvement_threshold_not_met"
    
    return "continue_optimization"

async def final_optimization_node(state: IterativeOptimizationState) -> IterativeOptimizationState:
    """æœ€ç»ˆä¼˜åŒ–èŠ‚ç‚¹"""
    
    solution = state["current_solution"]
    history = state["optimization_history"]
    
    # ç”Ÿæˆä¼˜åŒ–æŠ¥å‘Š
    llm = ChatOpenAI(model="gpt-4")
    prompt = f"""
    åŸºäºä¼˜åŒ–å†å²ï¼Œç”Ÿæˆæœ€ç»ˆä¼˜åŒ–æŠ¥å‘Šï¼š
    
    æœ€ç»ˆè§£å†³æ–¹æ¡ˆï¼š{solution}
    ä¼˜åŒ–å†å²ï¼š{history}
    
    è¯·æ€»ç»“ï¼š
    1. ä¼˜åŒ–è¿‡ç¨‹
    2. å…³é”®æ”¹è¿›ç‚¹
    3. æœ€ç»ˆæ–¹æ¡ˆä¼˜åŠ¿
    4. å®æ–½å»ºè®®
    """
    
    response = await llm.ainvoke(prompt)
    
    return {
        **state,
        "final_report": response.content
    }

def create_iterative_optimization_workflow():
    """åˆ›å»ºè¿­ä»£ä¼˜åŒ–å·¥ä½œæµ"""
    
    workflow = StateGraph(IterativeOptimizationState)
    
    workflow.add_node("solution_generator", solution_generator_node)
    workflow.add_node("quality_evaluator", quality_evaluator_node)
    workflow.add_node("final_optimization", final_optimization_node)
    
    # è®¾ç½®å¾ªç¯æµç¨‹
    workflow.add_edge("solution_generator", "quality_evaluator")
    
    workflow.add_conditional_edges(
        "quality_evaluator",
        should_continue_optimization,
        {
            "continue_optimization": "solution_generator",
            "max_iterations_reached": "final_optimization",
            "quality_threshold_reached": "final_optimization",
            "improvement_threshold_not_met": "final_optimization"
        }
    )
    
    workflow.add_edge("final_optimization", END)
    workflow.set_entry_point("solution_generator")
    
    return workflow.compile()

# ä½¿ç”¨ç¤ºä¾‹
async def run_iterative_optimization():
    """è¿è¡Œè¿­ä»£ä¼˜åŒ–"""
    
    app = create_iterative_optimization_workflow()
    
    initial_state = {
        "problem_description": "è®¾è®¡ä¸€ä¸ªé«˜æ•ˆçš„å®¢æˆ·æœåŠ¡ç³»ç»Ÿï¼Œèƒ½å¤Ÿå¤„ç†å¤šç§ç±»å‹çš„å®¢æˆ·æŸ¥è¯¢",
        "current_solution": "",
        "iteration_count": 0,
        "max_iterations": 5,
        "quality_score": 0.0,
        "improvement_threshold": 0.5,
        "optimization_history": []
    }
    
    result = await app.ainvoke(initial_state)
    
    print(f"ä¼˜åŒ–å®Œæˆï¼Œå…±è¿›è¡Œ{result['iteration_count']}æ¬¡è¿­ä»£")
    print(f"æœ€ç»ˆè´¨é‡è¯„åˆ†ï¼š{result['quality_score']}")
    print(f"æœ€ç»ˆè§£å†³æ–¹æ¡ˆï¼š{result['current_solution']}")
    
    return result
```

### 2.4 é”™è¯¯å¤„ç†ä¸æ¢å¤æœºåˆ¶

åœ¨å¤æ‚çš„å¤šæ™ºèƒ½ä½“å·¥ä½œæµä¸­ï¼Œé”™è¯¯å¤„ç†å’Œæ¢å¤æœºåˆ¶è‡³å…³é‡è¦ã€‚ä¼ä¸šçº§åº”ç”¨éœ€è¦å…·å¤‡å®Œå–„çš„å®¹é”™èƒ½åŠ›å’Œè‡ªåŠ¨æ¢å¤æœºåˆ¶ã€‚

> **ğŸ’¡ å®é™…ä»£ç å‚è€ƒ**ï¼šå®Œæ•´çš„é”™è¯¯å¤„ç†å’ŒçŠ¶æ€ç®¡ç†å®ç°å¯å‚è€ƒé¡¹ç›®ä¸­çš„ `langgraph_workflow.py` æ–‡ä»¶ï¼Œè¯¥æ–‡ä»¶æä¾›äº†ä¼ä¸šçº§çš„å¼‚å¸¸å¤„ç†ã€çŠ¶æ€éªŒè¯å’Œè‡ªåŠ¨æ¢å¤åŠŸèƒ½ã€‚

#### 2.4.1 ä¼ä¸šçº§é”™è¯¯å¤„ç†ç­–ç•¥

| é”™è¯¯ç±»å‹ | å¤„ç†ç­–ç•¥ | å®ç°æ–¹å¼ | é€‚ç”¨åœºæ™¯ | ä¼ä¸šçº§å¢å¼º |
|---------|---------|---------|---------|------------|
| **ç½‘ç»œé”™è¯¯** | é‡è¯•æœºåˆ¶ | æŒ‡æ•°é€€é¿ | APIè°ƒç”¨å¤±è´¥ | æ™ºèƒ½é‡è¯•ã€ç†”æ–­å™¨ |
| **æ•°æ®é”™è¯¯** | æ•°æ®æ¸…æ´— | å¼‚å¸¸å€¼å¤„ç† | è¾“å…¥æ•°æ®å¼‚å¸¸ | æ•°æ®éªŒè¯ã€è‡ªåŠ¨ä¿®å¤ |
| **é€»è¾‘é”™è¯¯** | å›æ»šé‡è¯• | çŠ¶æ€æ¢å¤ | ä¸šåŠ¡é€»è¾‘é”™è¯¯ | æ™ºèƒ½å›æ»šã€ç‰ˆæœ¬æ§åˆ¶ |
| **èµ„æºé”™è¯¯** | é™çº§å¤„ç† | å¤‡ç”¨æ–¹æ¡ˆ | èµ„æºä¸è¶³ | åŠ¨æ€æ‰©å®¹ã€è´Ÿè½½å‡è¡¡ |
| **è¶…æ—¶é”™è¯¯** | å¼‚æ­¥å¤„ç† | ä»»åŠ¡é˜Ÿåˆ— | é•¿æ—¶é—´ä»»åŠ¡ | åˆ†å¸ƒå¼å¤„ç†ã€è¿›åº¦è¿½è¸ª |

#### 2.4.2 ä¼ä¸šçº§é”™è¯¯å¤„ç†å®ç°

```python
# åŸºäºé¡¹ç›® multi_agent_system/src/workflows/langgraph_workflow.py çš„å¢å¼ºå®ç°
from typing import Optional, Dict, Any, List
import traceback
import logging
from datetime import datetime, timedelta
from enum import Enum
import asyncio
import json

class ErrorSeverity(Enum):
    """é”™è¯¯ä¸¥é‡ç¨‹åº¦"""
    LOW = "low"
    MEDIUM = "medium"
    HIGH = "high"
    CRITICAL = "critical"

class RecoveryStrategy(Enum):
    """æ¢å¤ç­–ç•¥"""
    RETRY = "retry"
    FALLBACK = "fallback"
    SKIP = "skip"
    ABORT = "abort"

@dataclass
class ErrorContext:
    """é”™è¯¯ä¸Šä¸‹æ–‡"""
    error_id: str
    timestamp: datetime
    node_name: str
    error_type: str
    error_message: str
    severity: ErrorSeverity
    recoverable: bool
    retry_count: int
    max_retries: int
    recovery_strategy: RecoveryStrategy
    stack_trace: str
    system_state: Dict[str, Any]

class EnterpriseWorkflowState(TypedDict):
    """ä¼ä¸šçº§å·¥ä½œæµçŠ¶æ€"""
    input_data: str
    processing_steps: List[str]
    error_log: Annotated[List[ErrorContext], operator.add]
    retry_count: int
    max_retries: int
    fallback_used: bool
    final_result: str
    performance_metrics: Dict[str, Any]
    circuit_breaker_status: Dict[str, bool]
    health_check_results: Dict[str, Any]

class CircuitBreaker:
    """ç†”æ–­å™¨å®ç°"""
    
    def __init__(self, failure_threshold: int = 5, timeout: int = 60):
        self.failure_threshold = failure_threshold
        self.timeout = timeout
        self.failure_count = 0
        self.last_failure_time = None
        self.state = "CLOSED"  # CLOSED, OPEN, HALF_OPEN
    
    def call(self, func, *args, **kwargs):
        """æ‰§è¡Œå‡½æ•°è°ƒç”¨ï¼Œå¸¦ç†”æ–­ä¿æŠ¤"""
        if self.state == "OPEN":
            if datetime.now() - self.last_failure_time > timedelta(seconds=self.timeout):
                self.state = "HALF_OPEN"
            else:
                raise Exception("Circuit breaker is OPEN")
        
        try:
            result = func(*args, **kwargs)
            if self.state == "HALF_OPEN":
                self.state = "CLOSED"
                self.failure_count = 0
            return result
        except Exception as e:
            self.failure_count += 1
            self.last_failure_time = datetime.now()
            
            if self.failure_count >= self.failure_threshold:
                self.state = "OPEN"
            
            raise e

class EnterpriseErrorHandler:
    """ä¼ä¸šçº§é”™è¯¯å¤„ç†å™¨"""
    
    def __init__(self):
        self.circuit_breakers: Dict[str, CircuitBreaker] = {}
        self.error_patterns: Dict[str, int] = {}
        self.recovery_strategies: Dict[str, RecoveryStrategy] = {}
        self.logger = logging.getLogger("EnterpriseErrorHandler")
    
    def handle_error(self, error: Exception, context: Dict[str, Any]) -> ErrorContext:
        """å¤„ç†é”™è¯¯å¹¶ç”Ÿæˆé”™è¯¯ä¸Šä¸‹æ–‡"""
        
        error_id = str(uuid.uuid4())
        node_name = context.get("node_name", "unknown")
        
        # åˆ†æé”™è¯¯ç±»å‹å’Œä¸¥é‡ç¨‹åº¦
        error_type, severity = self._analyze_error(error)
        
        # ç¡®å®šæ¢å¤ç­–ç•¥
        recovery_strategy = self._determine_recovery_strategy(
            error_type, severity, context
        )
        
        error_context = ErrorContext(
            error_id=error_id,
            timestamp=datetime.now(),
            node_name=node_name,
            error_type=error_type,
            error_message=str(error),
            severity=severity,
            recoverable=recovery_strategy != RecoveryStrategy.ABORT,
            retry_count=context.get("retry_count", 0),
            max_retries=context.get("max_retries", 3),
            recovery_strategy=recovery_strategy,
            stack_trace=traceback.format_exc(),
            system_state=self._capture_system_state()
        )
        
        # è®°å½•é”™è¯¯æ¨¡å¼
        self._record_error_pattern(error_type)
        
        # å‘é€å‘Šè­¦ï¼ˆå¦‚æœæ˜¯é«˜ä¸¥é‡ç¨‹åº¦é”™è¯¯ï¼‰
        if severity in [ErrorSeverity.HIGH, ErrorSeverity.CRITICAL]:
            self._send_alert(error_context)
        
        return error_context
    
    def _analyze_error(self, error: Exception) -> tuple[str, ErrorSeverity]:
        """åˆ†æé”™è¯¯ç±»å‹å’Œä¸¥é‡ç¨‹åº¦"""
        error_type = type(error).__name__
        
        # æ ¹æ®é”™è¯¯ç±»å‹ç¡®å®šä¸¥é‡ç¨‹åº¦
        severity_mapping = {
            "ConnectionError": ErrorSeverity.MEDIUM,
            "TimeoutError": ErrorSeverity.MEDIUM,
            "ValidationError": ErrorSeverity.LOW,
            "AuthenticationError": ErrorSeverity.HIGH,
            "SystemError": ErrorSeverity.CRITICAL,
            "MemoryError": ErrorSeverity.CRITICAL,
        }
        
        severity = severity_mapping.get(error_type, ErrorSeverity.MEDIUM)
        return error_type, severity
    
    def _determine_recovery_strategy(self, error_type: str, severity: ErrorSeverity, 
                                   context: Dict[str, Any]) -> RecoveryStrategy:
        """ç¡®å®šæ¢å¤ç­–ç•¥"""
        
        retry_count = context.get("retry_count", 0)
        max_retries = context.get("max_retries", 3)
        
        # åŸºäºé”™è¯¯ç±»å‹å’Œä¸¥é‡ç¨‹åº¦çš„ç­–ç•¥çŸ©é˜µ
        if severity == ErrorSeverity.CRITICAL:
            return RecoveryStrategy.ABORT
        
        if error_type in ["ConnectionError", "TimeoutError"] and retry_count < max_retries:
            return RecoveryStrategy.RETRY
        
        if error_type in ["ValidationError", "DataError"]:
            return RecoveryStrategy.FALLBACK
        
        if retry_count >= max_retries:
            return RecoveryStrategy.FALLBACK
        
        return RecoveryStrategy.RETRY
    
    def _capture_system_state(self) -> Dict[str, Any]:
        """æ•è·ç³»ç»ŸçŠ¶æ€"""
        return {
            "timestamp": datetime.now().isoformat(),
            "memory_usage": psutil.virtual_memory().percent,
            "cpu_usage": psutil.cpu_percent(),
            "disk_usage": psutil.disk_usage('/').percent,
            "active_connections": len(psutil.net_connections()),
        }
    
    def _record_error_pattern(self, error_type: str):
        """è®°å½•é”™è¯¯æ¨¡å¼"""
        self.error_patterns[error_type] = self.error_patterns.get(error_type, 0) + 1
    
    def _send_alert(self, error_context: ErrorContext):
        """å‘é€å‘Šè­¦"""
        alert_message = {
            "error_id": error_context.error_id,
            "severity": error_context.severity.value,
            "node_name": error_context.node_name,
            "error_message": error_context.error_message,
            "timestamp": error_context.timestamp.isoformat()
        }
        
        # è¿™é‡Œå¯ä»¥é›†æˆå®é™…çš„å‘Šè­¦ç³»ç»Ÿ
        self.logger.critical(f"High severity error detected: {json.dumps(alert_message)}")

async def enterprise_risky_processing_node(state: EnterpriseWorkflowState) -> EnterpriseWorkflowState:
    """ä¼ä¸šçº§é£é™©å¤„ç†èŠ‚ç‚¹"""
    
    input_data = state["input_data"]
    retry_count = state["retry_count"]
    error_handler = EnterpriseErrorHandler()
    
    try:
        # å¥åº·æ£€æŸ¥
        health_status = await perform_health_check()
        
        # ä½¿ç”¨ç†”æ–­å™¨ä¿æŠ¤
        circuit_breaker = error_handler.circuit_breakers.get("processing", CircuitBreaker())
        
        def risky_operation():
            if "error" in input_data.lower() and retry_count < 2:
                raise ConnectionError("Simulated network error")
            
            # æ¨¡æ‹Ÿå¤„ç†
            return f"Processed: {input_data}"
        
        result = circuit_breaker.call(risky_operation)
        
        return {
            **state,
            "processing_steps": [f"Successfully processed: {result}"],
            "final_result": result,
            "health_check_results": health_status,
            "circuit_breaker_status": {"processing": circuit_breaker.state}
        }
        
    except Exception as e:
        # ä½¿ç”¨ä¼ä¸šçº§é”™è¯¯å¤„ç†
        error_context = error_handler.handle_error(e, {
            "node_name": "enterprise_risky_processing",
            "retry_count": retry_count,
            "max_retries": state["max_retries"]
        })
        
        return {
            **state,
            "error_log": [error_context],
            "retry_count": retry_count + 1,
            "circuit_breaker_status": {"processing": "OPEN"}
        }

async def perform_health_check() -> Dict[str, Any]:
    """æ‰§è¡Œå¥åº·æ£€æŸ¥"""
    return {
        "database_connection": "healthy",
        "external_api": "healthy",
        "memory_usage": "normal",
        "response_time": "acceptable"
    }

def enterprise_should_retry_or_fallback(state: EnterpriseWorkflowState) -> str:
    """ä¼ä¸šçº§é‡è¯•æˆ–é™çº§å†³ç­–"""
    
    error_log = state["error_log"]
    circuit_breaker_status = state.get("circuit_breaker_status", {})
    
    if not error_log:
        return "success"
    
    latest_error = error_log[-1]
    
    # æ£€æŸ¥ç†”æ–­å™¨çŠ¶æ€
    if circuit_breaker_status.get("processing") == "OPEN":
        return "fallback"
    
    # åŸºäºæ¢å¤ç­–ç•¥å†³ç­–
    if latest_error.recovery_strategy == RecoveryStrategy.RETRY:
        return "retry"
    elif latest_error.recovery_strategy == RecoveryStrategy.FALLBACK:
        return "fallback"
    elif latest_error.recovery_strategy == RecoveryStrategy.ABORT:
        return "abort"
    else:
        return "success"
```

> **ğŸ”§ ä¼ä¸šçº§ç‰¹æ€§**ï¼š
>
> - **æ™ºèƒ½ç†”æ–­å™¨**ï¼šé˜²æ­¢çº§è”æ•…éšœ
> - **é”™è¯¯æ¨¡å¼è¯†åˆ«**ï¼šè‡ªåŠ¨å­¦ä¹ é”™è¯¯è§„å¾‹
> - **å¤šçº§å‘Šè­¦æœºåˆ¶**ï¼šåŠæ—¶å“åº”ä¸¥é‡é”™è¯¯
> - **ç³»ç»ŸçŠ¶æ€ç›‘æ§**ï¼šå…¨é¢çš„å¥åº·æ£€æŸ¥
> - **è‡ªé€‚åº”æ¢å¤ç­–ç•¥**ï¼šåŸºäºé”™è¯¯ç±»å‹æ™ºèƒ½é€‰æ‹©æ¢å¤æ–¹æ¡ˆ

#### 2.4.2 é”™è¯¯å¤„ç†å®ç°

```python
from typing import Optional
import traceback
import logging

class RobustWorkflowState(TypedDict):
    input_data: str
    processing_steps: List[str]
    error_log: Annotated[List[dict], operator.add]
    retry_count: int
    max_retries: int
    fallback_used: bool
    final_result: str

class WorkflowError(Exception):
    """å·¥ä½œæµé”™è¯¯åŸºç±»"""
    def __init__(self, message: str, error_type: str, recoverable: bool = True):
        super().__init__(message)
        self.error_type = error_type
        self.recoverable = recoverable

async def risky_processing_node(state: RobustWorkflowState) -> RobustWorkflowState:
    """å¯èƒ½å‡ºé”™çš„å¤„ç†èŠ‚ç‚¹"""
    
    input_data = state["input_data"]
    retry_count = state["retry_count"]
    
    try:
        # æ¨¡æ‹Ÿå¯èƒ½å¤±è´¥çš„å¤„ç†
        if "error" in input_data.lower() and retry_count < 2:
            raise WorkflowError(
                "Processing failed due to invalid input",
                "DATA_ERROR",
                recoverable=True
            )
        
        # æ­£å¸¸å¤„ç†
        llm = ChatOpenAI(model="gpt-4")
        response = await llm.ainvoke(f"å¤„ç†æ•°æ®ï¼š{input_data}")
        
        return {
            **state,
            "processing_steps": [f"Step completed: {response.content[:100]}..."],
            "final_result": response.content
        }
        
    except WorkflowError as e:
        # è®°å½•é”™è¯¯
        error_entry = {
            "timestamp": datetime.now().isoformat(),
            "error_type": e.error_type,
            "error_message": str(e),
            "retry_count": retry_count,
            "recoverable": e.recoverable,
            "stack_trace": traceback.format_exc()
        }
        
        return {
            **state,
            "error_log": [error_entry],
            "retry_count": retry_count + 1
        }
    
    except Exception as e:
        # æœªé¢„æœŸçš„é”™è¯¯
        error_entry = {
            "timestamp": datetime.now().isoformat(),
            "error_type": "UNEXPECTED_ERROR",
            "error_message": str(e),
            "retry_count": retry_count,
            "recoverable": False,
            "stack_trace": traceback.format_exc()
        }
        
        return {
            **state,
            "error_log": [error_entry],
            "retry_count": retry_count + 1
        }

async def fallback_processing_node(state: RobustWorkflowState) -> RobustWorkflowState:
    """å¤‡ç”¨å¤„ç†èŠ‚ç‚¹"""
    
    input_data = state["input_data"]
    
    # ä½¿ç”¨ç®€åŒ–çš„å¤„ç†é€»è¾‘
    llm = ChatOpenAI(model="gpt-3.5-turbo")
    prompt = f"ä½¿ç”¨ç®€åŒ–æ–¹å¼å¤„ç†ï¼š{input_data}"
    response = await llm.ainvoke(prompt)
    
    return {
        **state,
        "processing_steps": ["Fallback processing completed"],
        "fallback_used": True,
        "final_result": f"[Fallback] {response.content}"
    }

async def error_analysis_node(state: RobustWorkflowState) -> RobustWorkflowState:
    """é”™è¯¯åˆ†æèŠ‚ç‚¹"""
    
    error_log = state["error_log"]
    
    if error_log:
        # åˆ†æé”™è¯¯æ¨¡å¼
        error_types = [entry["error_type"] for entry in error_log]
        most_common_error = max(set(error_types), key=error_types.count)
        
        analysis = {
            "total_errors": len(error_log),
            "most_common_error": most_common_error,
            "error_pattern": error_types,
            "recommendations": generate_error_recommendations(error_log)
        }
        
        return {
            **state,
            "error_analysis": analysis
        }
    
    return state

def generate_error_recommendations(error_log: List[dict]) -> List[str]:
    """ç”Ÿæˆé”™è¯¯å¤„ç†å»ºè®®"""
    recommendations = []
    
    error_types = [entry["error_type"] for entry in error_log]
    
    if "DATA_ERROR" in error_types:
        recommendations.append("å»ºè®®å¢å¼ºè¾“å…¥æ•°æ®éªŒè¯")
    
    if "NETWORK_ERROR" in error_types:
        recommendations.append("å»ºè®®å®æ–½æ›´å¼ºçš„é‡è¯•æœºåˆ¶")
    
    if len(error_log) > 3:
        recommendations.append("å»ºè®®æ£€æŸ¥ç³»ç»Ÿç¨³å®šæ€§")
    
    return recommendations

def should_retry_or_fallback(state: RobustWorkflowState) -> str:
    """å†³å®šé‡è¯•è¿˜æ˜¯ä½¿ç”¨å¤‡ç”¨æ–¹æ¡ˆ"""
    
    retry_count = state["retry_count"]
    max_retries = state["max_retries"]
    error_log = state["error_log"]
    
    # æ£€æŸ¥æ˜¯å¦æœ‰é”™è¯¯
    if not error_log:
        return "success"
    
    # æ£€æŸ¥æœ€æ–°é”™è¯¯æ˜¯å¦å¯æ¢å¤
    latest_error = error_log[-1]
    
    if not latest_error["recoverable"]:
        return "fallback"
    
    if retry_count < max_retries:
        return "retry"
    else:
        return "fallback"

def create_robust_workflow():
    """åˆ›å»ºå¥å£®çš„å·¥ä½œæµ"""
    
    workflow = StateGraph(RobustWorkflowState)
    
    workflow.add_node("risky_processing", risky_processing_node)
    workflow.add_node("fallback_processing", fallback_processing_node)
    workflow.add_node("error_analysis", error_analysis_node)
    
    # è®¾ç½®é”™è¯¯å¤„ç†æµç¨‹
    workflow.add_conditional_edges(
        "risky_processing",
        should_retry_or_fallback,
        {
            "success": "error_analysis",
            "retry": "risky_processing",
            "fallback": "fallback_processing"
        }
    )
    
    workflow.add_edge("fallback_processing", "error_analysis")
    workflow.add_edge("error_analysis", END)
    workflow.set_entry_point("risky_processing")
    
    return workflow.compile()

# ä½¿ç”¨ç¤ºä¾‹
async def test_error_handling():
    """æµ‹è¯•é”™è¯¯å¤„ç†"""
    
    app = create_robust_workflow()
    
    test_cases = [
        "æ­£å¸¸æ•°æ®å¤„ç†",
        "åŒ…å«errorçš„æ•°æ®",  # ä¼šè§¦å‘é”™è¯¯
        "å¦ä¸€ä¸ªæ­£å¸¸æ•°æ®"
    ]
    
    for test_data in test_cases:
        initial_state = {
            "input_data": test_data,
            "processing_steps": [],
            "error_log": [],
            "retry_count": 0,
            "max_retries": 3,
            "fallback_used": False,
            "final_result": ""
        }
        
        result = await app.ainvoke(initial_state)
        
        print(f"è¾“å…¥ï¼š{test_data}")
        print(f"ç»“æœï¼š{result['final_result']}")
        print(f"é”™è¯¯æ¬¡æ•°ï¼š{len(result['error_log'])}")
        print(f"ä½¿ç”¨å¤‡ç”¨æ–¹æ¡ˆï¼š{result['fallback_used']}")
        print("-" * 50)
```

### 2.5 æ€§èƒ½ä¼˜åŒ–æŠ€å·§

#### 2.5.1 ä¼˜åŒ–ç­–ç•¥æ€»ç»“

| ä¼˜åŒ–ç»´åº¦ | å…·ä½“æŠ€å·§ | å®ç°æ–¹æ³• | æ€§èƒ½æå‡ |
|---------|---------|---------|----------|
| **å¹¶å‘ä¼˜åŒ–** | å¼‚æ­¥å¤„ç†ã€å¹¶è¡Œæ‰§è¡Œ | asyncioã€å¤šçº¿ç¨‹ | 2-5å€ |
| **ç¼“å­˜ä¼˜åŒ–** | ç»“æœç¼“å­˜ã€çŠ¶æ€ç¼“å­˜ | Redisã€å†…å­˜ç¼“å­˜ | 3-10å€ |
| **èµ„æºä¼˜åŒ–** | è¿æ¥æ± ã€å¯¹è±¡å¤ç”¨ | è¿æ¥ç®¡ç†ã€å¯¹è±¡æ±  | 20-50% |
| **ç®—æ³•ä¼˜åŒ–** | æ™ºèƒ½è·¯ç”±ã€è´Ÿè½½å‡è¡¡ | å¯å‘å¼ç®—æ³• | 30-100% |

#### 2.5.2 æ€§èƒ½ç›‘æ§å®ç°

```python
import time
import psutil
from dataclasses import dataclass
from typing import Dict, Any

@dataclass
class PerformanceMetrics:
    """æ€§èƒ½æŒ‡æ ‡"""
    execution_time: float
    memory_usage: float
    cpu_usage: float
    node_execution_times: Dict[str, float]
    throughput: float

class PerformanceMonitor:
    """æ€§èƒ½ç›‘æ§å™¨"""
    
    def __init__(self):
        self.metrics_history: List[PerformanceMetrics] = []
        self.node_timings: Dict[str, List[float]] = {}
    
    async def monitor_workflow_execution(self, workflow_func, *args, **kwargs):
        """ç›‘æ§å·¥ä½œæµæ‰§è¡Œ"""
        
        start_time = time.time()
        start_memory = psutil.Process().memory_info().rss / 1024 / 1024  # MB
        start_cpu = psutil.cpu_percent()
        
        # æ‰§è¡Œå·¥ä½œæµ
        result = await workflow_func(*args, **kwargs)
        
        end_time = time.time()
        end_memory = psutil.Process().memory_info().rss / 1024 / 1024  # MB
        end_cpu = psutil.cpu_percent()
        
        # è®¡ç®—æŒ‡æ ‡
        execution_time = end_time - start_time
        memory_usage = end_memory - start_memory
        cpu_usage = end_cpu - start_cpu
        
        metrics = PerformanceMetrics(
            execution_time=execution_time,
            memory_usage=memory_usage,
            cpu_usage=cpu_usage,
            node_execution_times={},
            throughput=1.0 / execution_time if execution_time > 0 else 0
        )
        
        self.metrics_history.append(metrics)
        
        return result, metrics
    
    def get_performance_report(self) -> Dict[str, Any]:
        """ç”Ÿæˆæ€§èƒ½æŠ¥å‘Š"""
        
        if not self.metrics_history:
            return {"error": "No metrics available"}
        
        recent_metrics = self.metrics_history[-10:]  # æœ€è¿‘10æ¬¡æ‰§è¡Œ
        
        avg_execution_time = sum(m.execution_time for m in recent_metrics) / len(recent_metrics)
        avg_memory_usage = sum(m.memory_usage for m in recent_metrics) / len(recent_metrics)
        avg_throughput = sum(m.throughput for m in recent_metrics) / len(recent_metrics)
        
        return {
            "average_execution_time": avg_execution_time,
            "average_memory_usage": avg_memory_usage,
            "average_throughput": avg_throughput,
            "total_executions": len(self.metrics_history),
            "performance_trend": self.analyze_performance_trend()
        }
    
    def analyze_performance_trend(self) -> str:
        """åˆ†ææ€§èƒ½è¶‹åŠ¿"""
        
        if len(self.metrics_history) < 5:
            return "æ•°æ®ä¸è¶³ï¼Œæ— æ³•åˆ†æè¶‹åŠ¿"
        
        recent_times = [m.execution_time for m in self.metrics_history[-5:]]
        earlier_times = [m.execution_time for m in self.metrics_history[-10:-5]]
        
        recent_avg = sum(recent_times) / len(recent_times)
        earlier_avg = sum(earlier_times) / len(earlier_times)
        
        if recent_avg < earlier_avg * 0.9:
            return "æ€§èƒ½æå‡"
        elif recent_avg > earlier_avg * 1.1:
            return "æ€§èƒ½ä¸‹é™"
        else:
            return "æ€§èƒ½ç¨³å®š"

# ä½¿ç”¨ç¤ºä¾‹
async def performance_test():
    """æ€§èƒ½æµ‹è¯•ç¤ºä¾‹"""
    
    monitor = PerformanceMonitor()
    
    # æµ‹è¯•ä¸åŒçš„å·¥ä½œæµ
    workflows = [
        create_customer_service_workflow(),
        create_parallel_analysis_workflow(),
        create_iterative_optimization_workflow()
    ]
    
    for i, workflow in enumerate(workflows):
        print(f"æµ‹è¯•å·¥ä½œæµ {i+1}")
        
        # æ‰§è¡Œå¤šæ¬¡æµ‹è¯•
        for j in range(5):
            test_state = {
                "input_data": f"æµ‹è¯•æ•°æ® {j+1}",
                # ... å…¶ä»–åˆå§‹çŠ¶æ€
            }
            
            result, metrics = await monitor.monitor_workflow_execution(
                workflow.ainvoke, test_state
            )
            
            print(f"  æ‰§è¡Œ {j+1}: {metrics.execution_time:.2f}s, "
                  f"å†…å­˜: {metrics.memory_usage:.2f}MB")
        
        # ç”ŸæˆæŠ¥å‘Š
        report = monitor.get_performance_report()
        print(f"  å¹³å‡æ‰§è¡Œæ—¶é—´: {report['average_execution_time']:.2f}s")
        print(f"  æ€§èƒ½è¶‹åŠ¿: {report['performance_trend']}")
        print("-" * 50)
```

### 2.6 å®è·µç¯èŠ‚

#### 2.6.1 ç»¼åˆç»ƒä¹ ï¼šæ„å»ºä¼ä¸šçº§å¤šæ™ºèƒ½ä½“å®¢æœç³»ç»Ÿ

```python
# ä¼ä¸šçº§å®¢æœç³»ç»Ÿå®Œæ•´å®ç°
class EnterpriseCustomerServiceState(TypedDict):
    customer_id: str
    query: str
    priority: str
    category: str
    assigned_agent: str
    conversation_history: Annotated[List[dict], operator.add]
    resolution_status: str
    satisfaction_score: float
    escalation_level: int
    processing_time: float

# å®ç°å®Œæ•´çš„ä¼ä¸šçº§å®¢æœå·¥ä½œæµ
def create_enterprise_customer_service():
    """åˆ›å»ºä¼ä¸šçº§å®¢æœç³»ç»Ÿ"""
    
    workflow = StateGraph(EnterpriseCustomerServiceState)
    
    # æ·»åŠ æ‰€æœ‰å¿…è¦çš„èŠ‚ç‚¹
    workflow.add_node("intake_classification", intake_classification_node)
    workflow.add_node("priority_assessment", priority_assessment_node)
    workflow.add_node("agent_assignment", agent_assignment_node)
    workflow.add_node("primary_response", primary_response_node)
    workflow.add_node("quality_check", quality_check_node)
    workflow.add_node("escalation_handler", escalation_handler_node)
    workflow.add_node("satisfaction_survey", satisfaction_survey_node)
    workflow.add_node("case_closure", case_closure_node)
    
    # è®¾ç½®å¤æ‚çš„æ¡ä»¶æµç¨‹
    workflow.add_edge("intake_classification", "priority_assessment")
    workflow.add_edge("priority_assessment", "agent_assignment")
    workflow.add_edge("agent_assignment", "primary_response")
    
    workflow.add_conditional_edges(
        "primary_response",
        check_response_quality,
        {
            "approved": "satisfaction_survey",
            "needs_improvement": "quality_check",
            "escalate": "escalation_handler"
        }
    )
    
    workflow.add_edge("quality_check", "primary_response")
    workflow.add_edge("escalation_handler", "satisfaction_survey")
    workflow.add_edge("satisfaction_survey", "case_closure")
    workflow.add_edge("case_closure", END)
    
    workflow.set_entry_point("intake_classification")
    
    return workflow.compile()

# è¿è¡Œå®Œæ•´æµ‹è¯•
async def run_enterprise_test():
    """è¿è¡Œä¼ä¸šçº§æµ‹è¯•"""
    
    app = create_enterprise_customer_service()
    monitor = PerformanceMonitor()
    
    test_cases = [
        {
            "customer_id": "CUST001",
            "query": "æˆ‘çš„è´¦å•æœ‰é—®é¢˜ï¼Œè´¹ç”¨æ¯”é¢„æœŸé«˜å¾ˆå¤š",
            "priority": "",
            "category": "",
            "assigned_agent": "",
            "conversation_history": [],
            "resolution_status": "open",
            "satisfaction_score": 0.0,
            "escalation_level": 0,
            "processing_time": 0.0
        }
    ]
    
    for test_case in test_cases:
        result, metrics = await monitor.monitor_workflow_execution(
            app.ainvoke, test_case
        )
        
        print(f"å®¢æˆ·ID: {result['customer_id']}")
        print(f"æŸ¥è¯¢: {result['query']}")
        print(f"åˆ†é…æ™ºèƒ½ä½“: {result['assigned_agent']}")
        print(f"è§£å†³çŠ¶æ€: {result['resolution_status']}")
        print(f"æ»¡æ„åº¦: {result['satisfaction_score']}")
        print(f"å¤„ç†æ—¶é—´: {metrics.execution_time:.2f}s")
        print("-" * 50)
    
    # ç”Ÿæˆæ€§èƒ½æŠ¥å‘Š
    report = monitor.get_performance_report()
    print("æ€§èƒ½æŠ¥å‘Š:")
    print(f"å¹³å‡æ‰§è¡Œæ—¶é—´: {report['average_execution_time']:.2f}s")
    print(f"å¹³å‡ååé‡: {report['average_throughput']:.2f} requests/s")
```

è¿™ä¸ªç¬¬äºŒå¤©çš„åŸ¹è®­ææ–™æ¶µç›–äº†LangGraphæ¡†æ¶çš„é«˜çº§åº”ç”¨ï¼ŒåŒ…æ‹¬å¹¶è¡Œå¤„ç†ã€å¾ªç¯æ§åˆ¶ã€é”™è¯¯å¤„ç†å’Œæ€§èƒ½ä¼˜åŒ–ç­‰å…³é”®æŠ€æœ¯ï¼Œä¸ºå­¦å‘˜æä¾›äº†æ„å»ºä¼ä¸šçº§å¤šæ™ºèƒ½ä½“ç³»ç»Ÿçš„å®Œæ•´çŸ¥è¯†ä½“ç³»ã€‚
